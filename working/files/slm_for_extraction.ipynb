{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "404547fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "708145b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\gayat\\AppData\\Local\\Temp\\ipykernel_26028\\2871879265.py:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  df = pd.read_csv(\"G:\\My Drive\\projects\\data_science\\working\\dataset\\AI_Resume_Screening.csv\")\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"G:\\My Drive\\projects\\data_science\\working\\dataset\\AI_Resume_Screening.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95d32b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique skills: 14\n",
      "['C++', 'Cybersecurity', 'Deep Learning', 'Ethical Hacking', 'Java', 'Linux', 'Machine Learning', 'NLP', 'Networking', 'Python', 'Pytorch', 'React', 'SQL', 'TensorFlow']\n"
     ]
    }
   ],
   "source": [
    "all_skills = []\n",
    "for s in df[\"Skills\"].dropna():\n",
    "    skills = [skill.strip() for skill in s.split(\",\")]\n",
    "    all_skills.extend(skills)\n",
    "\n",
    "unique_skills = sorted(set(all_skills))\n",
    "\n",
    "print(f\"Total unique skills: {len(unique_skills)}\")\n",
    "print(unique_skills[:30]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "860e7818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\gayat\\AppData\\Local\\Temp\\ipykernel_26028\\1136566128.py:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  df_1 = pd.read_csv(\"G:\\My Drive\\projects\\data_science\\working\\dataset\\jobss.csv\")\n"
     ]
    }
   ],
   "source": [
    "df_1 = pd.read_csv(\"G:\\My Drive\\projects\\data_science\\working\\dataset\\jobss.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89a1be46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique skills: 2161\n",
      "['.NET', '.NET development', '.Net', '.net', '.net developer', '11g', '3D', '3D Modeling', '3G', '3GPP', '4G', 'ADFS', 'AO engagement', 'AP', 'API', 'API Testing', 'ASM', 'ASP', 'ASP.Net MVC', 'AWS', 'Accent', 'Account Assistant', 'Account Management', 'Accounting', 'Accounting Entries', 'Accounts', 'Accounts Executive', 'Accounts receivable', 'Action plan', 'Active Directory']\n"
     ]
    }
   ],
   "source": [
    "for s in df_1[\"Key Skills\"].dropna():\n",
    "    skills = [skill.strip() for skill in s.split(\"|\")]\n",
    "    all_skills.extend(skills)\n",
    "\n",
    "unique_skills = sorted(set(all_skills))\n",
    "\n",
    "print(f\"Total unique skills: {len(unique_skills)}\")\n",
    "print(unique_skills[:30]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90e40946",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv(r\"G:\\My Drive\\projects\\data_science\\working\\dataset\\resume_dataset_1200.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "002d42c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique skills: 2195\n",
      "['.NET', '.NET development', '.Net', '.net', '.net developer', '11g', '3D', '3D Modeling', '3G', '3GPP', '4G', 'ADFS', 'AO engagement', 'AP', 'API', 'API Testing', 'ASM', 'ASP', 'ASP.Net MVC', 'AWS', 'Accent', 'Account Assistant', 'Account Management', 'Accounting', 'Accounting Entries', 'Accounts', 'Accounts Executive', 'Accounts receivable', 'Action plan', 'Active Directory']\n"
     ]
    }
   ],
   "source": [
    "for s in df_2[\"Skills\"].dropna():\n",
    "    skills = [skill.strip() for skill in s.split(\",\")]\n",
    "    all_skills.extend(skills)\n",
    "\n",
    "unique_skills = sorted(set(all_skills))\n",
    "\n",
    "print(f\"Total unique skills: {len(unique_skills)}\")\n",
    "print(unique_skills[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "546ce4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\gayat\\AppData\\Local\\Temp\\ipykernel_26028\\90045147.py:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  df_3 = pd.read_csv(\"G:\\My Drive\\projects\\data_science\\working\\dataset\\stack_network_links.csv\")\n"
     ]
    }
   ],
   "source": [
    "df_3 = pd.read_csv(\"G:\\My Drive\\projects\\data_science\\working\\dataset\\stack_network_links.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3000e0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique skills: 2265\n",
      "['.NET', '.NET development', '.Net', '.net', '.net developer', '11g', '3D', '3D Modeling', '3G', '3GPP', '4G', 'ADFS', 'AO engagement', 'AP', 'API', 'API Testing', 'ASM', 'ASP', 'ASP.Net MVC', 'AWS', 'Accent', 'Account Assistant', 'Account Management', 'Accounting', 'Accounting Entries', 'Accounts', 'Accounts Executive', 'Accounts receivable', 'Action plan', 'Active Directory']\n"
     ]
    }
   ],
   "source": [
    "for s in df_3[\"source\"].dropna():\n",
    "    skills = [skill.strip() for skill in s.split(\",\")]\n",
    "    all_skills.extend(skills)\n",
    "\n",
    "unique_skills = sorted(set(all_skills))\n",
    "\n",
    "print(f\"Total unique skills: {len(unique_skills)}\")\n",
    "print(unique_skills[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3180699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = [x.lower() for x in unique_skills]\n",
    "skills = sorted(set(skills))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "488c8d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1865\n"
     ]
    }
   ],
   "source": [
    "print(len(skills))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37df5a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame({\"skill\":skills})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b215c271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\gayat\\AppData\\Local\\Temp\\ipykernel_26028\\68390848.py:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  final_df.to_csv(\"G:\\My Drive\\projects\\data_science\\working\\dataset\\skills.csv\", index = False)\n"
     ]
    }
   ],
   "source": [
    "final_df.to_csv(\"G:\\My Drive\\projects\\data_science\\working\\dataset\\skills.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22100da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a6bc43bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique skills collected: 1995\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "all_skills = []\n",
    "\n",
    "for i in range(1, 652):\n",
    "    path = f\"G:\\\\My Drive\\\\projects\\\\data_science\\\\working\\\\dataset\\\\second_\\\\resume_{i}.json\"\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        continue\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            resume = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    \n",
    "    if isinstance(resume, dict):\n",
    "        skills_dict = resume.get(\"skills\", {})\n",
    "        tech_skills = skills_dict.get(\"technical_skills\", [])\n",
    "        tools = skills_dict.get(\"tools_and_software\", [])\n",
    "\n",
    "        # Combine both lists\n",
    "        raw_skills = tech_skills + tools\n",
    "\n",
    "        # Clean each skill\n",
    "        for s in raw_skills:\n",
    "            # Replace newline and extra quotes\n",
    "            cleaned = re.sub(r'[\\n\"]', '', s).strip()\n",
    "            \n",
    "            # Sometimes multiple skills are stuck together with commas, split them\n",
    "            split_skills = [skill.strip() for skill in cleaned.split(',') if skill.strip()]\n",
    "            all_skills.extend(split_skills)\n",
    "\n",
    "# Remove duplicates\n",
    "all_skills = list(set(all_skills))\n",
    "\n",
    "print(f\"Total unique skills collected: {len(all_skills)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8f9facd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Trouble-Shooting', 'Vulnerability Management (FISMA', '12.2R', 'Cloud Computing (AWS)', 'ASM Administration', 'Replication', 'System Testing', 'Quest Central', 'Technical Documentation', 'Back end', 'SIP', 'db2patch', 'Adobe Acrobat Pro', 'Microsoft SQL Server 2005/2008', 'SQL Server Sqlplus', 'and Testing with Jenkins', 'Oracle 10g Administration', 'Digital Marketing Strategies', 'Fragmentation Issues', 'IBM AIX Servers', 'Redgate SQL Toolbelt', 'BACKUP AND RECOVERY', '2008', 'Westlaw', 'Pencil', 'UNIX', 'VMware Service Manager', 'Manhattan', 'VS CODE', 'Samba 4', 'Oracle Data Guard', 'Data Entry', 'VMware (4 years)', 'Oracle Application Server', 'Gulp', 'NACAR Branch server', 'Security Patch Application', 'Atlassian Confluence (4 years)', 'ACLs', 'InfoPath', 'Salesforce Automation', 'Frameworks', 'I2C', 'jquery (9 years)', 'Antivirus Software', 'Linux (Fedora/Ubuntu)', 'Oracle RDBMS 11g-12c', 'SSIS/DTS', 'KnexJS', 'MS Excel 2016 & 365 - Intermediate', 'WORDPRESS', 'GOLDEN GATE', 'Incident Response', 'INFORMATICA', 'Query Optimization', 'Access Control Lists (ACLs)', 'High Availability', 'Backup/Recovery Strategies', 'Oracle Active DataGuard', 'Oracle (11g', '// Additional tools not explicitly mentioned', 'Red Hat Linux 7.x', 'SQL Server (2012', 'System Monitor', 'T-SQL Certification', 'Network Monitoring Software', 'HP Quality Center', 'Others', 'Data Guard', 'and Replication Configurations', 'Prototyping', 'ExpressJS', 'Tools and Software: Toad', '2012)', 'Database Transformation Services', 'Linux (2.6.32)', 'Windows Operating Systems: MS-DOS', 'Database Security (TDE', 'TDE Encryption', 'Database Entry', 'HTML with CSS', 'Sql Server (2 years)', 'Linux/Unix Systems', \"DMV's\", 'Spotlight', 'Database Modeling', 'Cloud Infrastructure', 'Hummingbird', 'Sun', 'RMAN (Database Backup and Recovery Tool)', 'Always On Availability Group', 'UNIX Shell Scripting', 'High Availability Solutions (RAC)', 'IT Networking Fundamentals', 'JAMS (Job Activity Management System)', 'SecureVPN', 'Budgeting', 'Logrotate', 'Clinical Workflows', 'TRANSFORM', 'Operating Systems: Linux', 'EM Grid Control', 'Cisco routers/switches configuration', 'IAM', 'BMC Footprints Service Core', 'Windows Server (2003/2008/R2/2012)', 'Crucible', 'Oracle Database 11g/12c', 'Bitnami', 'Business Management', 'IMP', 'Oracle', 'Oracle RAC 10g', 'Lumedx Systems', 'Oracle GoldenGate Replication', 'Windows Performance Monitor', 'EXCEL (3 years)', 'DevOps Practices', 'ArcServe', 'MS SQL SERVER', 'In-Situ Observations', 'STATPACK', 'Oracle Databases (12c/11g/10g/9i/8i)', 'Disaster Recovery and Backup Strategies', 'Statspack/AWR', 'Data Visualization', 'Datapump Export/Import', 'QA Analysis', 'Systems Development Life Cycle (SDLC)', 'Azure)', 'Process Documentation', 'DynamoDB', 'SQL*PLUS', 'Amazon EBS', 'jQuery', 'Scripting Languages', 'Data Replication Strategies', 'Windows 7', 'Oracle OEM Grid Control', 'Agile Development', 'Oracle Tools', 'Red Hat Linux 6.5X', 'Quest Toad', 'User Access Control', 'PerfMon Analyzer', 'LibreOffice', 'Export/Import Tools', 'RAC and Grid Infrastructure with ASM', 'Visual Studio .NET', 'LaserPro', 'SAN', 'IDLE', 'Oracle Database Tools (SQL', 'OCA Certification', 'Oracle database management', 'SQL Server (2000', 'TOAD (Toast and Dialect)', 'Sametime', 'Database Performance Optimization', '12c Administration', 'SQL Optimization', 'Nintex', 'Network Troubleshooting', 'Cardiovascular Information Systems', 'EDraw', 'Fail-Over Clustering', 'Customer Service', 'Database Engine Tuning Advisor', 'Failover Clusters', 'SYBASE', 'Testing', 'GUANGO', 'NoSQL)', 'Billing/Collections', 'Cloning and Space Management', 'CodeDeploy', 'MS SQL Server', 'Flashback Technology', 'Computer Literate', 'Slack)', 'SQL databases', 'Amazon RDS', 'Oracle Applications', 'Oracle Enterprise Manager Grid Control', 'SQL Server 2012/2008 R2', 'Verint', 'Amazon Redshift', 'IIS', 'Wire shark', 'XCode', 'Quest Spotlight', 'Excel', 'Intrusion Detection Systems', 'Team collaboration', 'Web Hosting', 'Veritas Volume Manager', 'QDesigner', 'BASH', 'Azure Technology', 'DataGuard/Standby', 'Greenplum', 'Transportable Tablespace', 'Data Pump Export/Import', 'HP', 'Microsoft Azure', 'Cloud Administration (2 years)', 'Comptia', 'SSM', 'Customer Focus', 'Agile Scrum', 'FormAssembly', 'SharePlex', 'After Effects', 'ODOO', 'SQL*QueryAnalyzer', 'RMAN (Remote Manager)', 'SQL Management Studio', 'SCADA Systems Implementation', 'Security Life Cycle', 'Data Management and ETL Processes', 'Load (ETL) Tools', 'Paint 3D', 'Protegrity', 'Network Tools', 'ORACLE', 'Oracle Enterprise Manager (OEM)', 'Toad Data Analyzer', 'Java Development Kit (JDK)', 'Windows Server 2012 R2', 'Administrative Assistant', 'STATSPACK', 'Automation and deployment', 'Microsoft Office (PowerPoint', 'SQL Server Agent', 'Red Hat (RHEL)', 'z/ OS V1.7', 'Convio', 'Data Science', 'SQL development', 'Hummingbird Tivoli Monitoring', 'Security Testing Tools', 'DB2 z/OS Version 9', 'Golden Gate', '2008R2 & 2012', 'Application Server Administration (Apache Tomcat)', 'SpagoBI', 'R', 'AWR (Application Workload Reporter)', 'OKTA', 'MySQL', 'IBM DB2', 'Bash/Python Scripting', 'Express.js', 'SQL-Tuning Advisor', 'EC2', 'Ms SQL Server', 'AWS Solution Architect (2 years)', 'Oracle DBMS Administration', 'Microsoft Dynamics AX', 'JavaScript (9 years)', 'UC4 (Automic)', '11g', 'AWS (Amazon Web Services)', 'Cisco VPN', 'Trend Micro', 'SQL Server', 'SQL Tuning (9 years)', 'Flash', 'Property management', 'NoSQL databases', 'SRVCTL', 'Server support', 'Jenkins (4 years)', 'Amazon Web Services (IAM', 'Data Warehouse', 'Storyboarding', 'ASC X12 EDI', 'Web applications', 'VMware vSphere 5.5', 'Employee Relations', 'Nutanix Platform', 'database security', 'Communication Tools', 'Oracle databases', 'SIEM', 'Oracle RMAN', 'AIX (7.1)', 'Microsoft Word', 'Oracle DBA', 'VMware ESXi', 'SonicWall', 'AND LOAD', 'Process Control System Design', 'ArcGIS server', 'Rubrik', 'SQL Reporting Services (SSRS)', 'BI Development (SSIS', 'SHELL SCRIPTING', 'Microsoft', 'Multi-channel marketing platforms', 'OpenText', 'High Availability Configuration', 'Real Application Cluster', 'Wireshark', 'AWS Data Pipeline', 'Microsoft Windows Applications (Word', 'VSS', 'Database Migration to Cloud', 'SQL Dba', 'Qualitative Research Methods', 'Case Management', 'Twilio', 'NFS/CIFS/iSCSI Protocols', 'Metadata', 'Oracle 10/11g', 'Data Warehouse Management', 'Database administration skills (SQL', 'Asana', 'T-SQL', 'Oracle RAC', 'Microsoft SQL Server', 'GoTo Assist', 'Export/Import Utilities', '11g)', 'triggers', '2014', 'Cach�', 'SQL (2000', 'Backup Strategies', 'OEM (Oracle Enterprise Manager)', 'Github (4 years)', 'Disaster Recovery Planning', 'TSQL', 'HubSpot', 'Data Integration', 'Statistical Reporting', 'Sequelize', 'iMessenger', 'Web Application Development', 'Oracle Migration Workbench', 'Visio for Data Modeling', 'BGP', 'LUA', 'Client Relations', 'Elasticache', 'Mac OS', 'SQL Server 2008R2', 'Google Ads', 'DB2 Data Studio', 'Incident Resolution', 'Oracle and MS SQL Server expertise', 'ELK Log Monitoring', 'BigFix', 'ASP', 'Staff Training', 'Leadership', 'Microsoft SQL Server Access Database', 'Data Warehouse Optimization', 'DB Configuration Assistant', 'GitHub', 'DB2 Visualizer', 'Triggers', 'System Admin', 'ERWin', 'MS Excel', 'IXIA', 'Schema Management', 'Route 53', 'Ignite - SolarWinds', 'Estimation', 'Query Analyzer', 'Data architect', 'Database Creation Using DBCA', 'PHP (1 year)', 'Simulink', 'MySQL)', 'Oracle 11g/10g', 'vCenter and vCloud Director knowledge', 'AWS Certified Solutions Architect – Associate', 'Aqua Data Studio', 'Windows Server 2008/2008R2/2012', 'Database Maintenance', 'SyncApps', 'Backup strategies using RMAN', 'ServiceNow', 'PITR', 'R Commander (Less than 1 year)', 'Integration Services (SSIS)', 'Zapier', 'WebLogic Server Administration Console', 'Apsona', 'DHTML', 'Windows Server 2003-2012', 'OCP in Progress', 'Data analytics', 'ODBC', 'Performance', 'Active Directory', 'TestRail', 'ADMM', 'AlwaysOn', 'Spring', 'PostgreSQL', 'PUTTY', 'GPText', 'Requirements Analysis', 'UML', 'MapForce', 'Mirroring', 'Medisoft', 'Elasticsearch', 'Shopify', 'JSON', 'WDS', 'Scrum Tools', 'Windows applications', 'Database Design', 'MS Project', 'Disaster Recovery Implementation', 'Web Development', 'MICROSOFT OFFICE', 'Nessus', 'Access', 'VMware for Virtual Environment Administration', 'Project management', 'Training', 'AWS Migration', 'Remedy Tool', 'Log Miner', 'SQL*Plus Programming', 'OLTP', 'Advanced Microsoft Word and Excel', 'Oracle E-Business Suite Training', 'Data management', 'Transform', 'Cloud Platforms', 'Microsoft Business Intelligence Development Studio', 'Third-party tools: Lite Speed', 'EXTRACT', 'Hand Tools', 'Reporting Services', \"Raiser's Edge\", 'Korn)', 'UNIX (Solaris 8', 'Power Point', 'FileMaker', 'Idera SQL Diagnostics', 'SQL Integration Services (SSIS)', 'Bitbucket', 'Microsoft SQL Server (2000', 'DBMS', 'Team-Oriented Approach', 'VMware ESXi/ESX', 'Management', 'Grid Control', 'Restore and Recovery Options', 'Putty Terminal Emulator', 'SQLT', 'JAVASCRIPT', 'DB2 UDB Administration', 'ASPEN Plus', 'MS SQL Server (2003-2014)', 'LAMP', 'Knowledge Base', 'Database Capacity Planning', 'RDBMS', 'VPNs', '4Sight', 'OEM Grid Control', 'BCP', 'Wireframing', 'PerfMon', 'EulerScript', 'Windows Server Management', 'Server Virtualization', 'Confluence', 'Outlook', 'CRM (SugarCRM', 'EXP', '9 & 10)', 'VLAN', 'PL/SQL)', 'Stripe', 'Jenkins', 'RAC and Data Guard Configuration', 'McAfee VirusScan Enterprise', 'Event Planning', 'TKPROF', 'NoSQL', 'Networking (CCNA certification in progress)', 'Erwin (Reverse Engineering Tool)', 'Database Security Management', 'and merge)', 'Database Security', 'Meditech', 'Advanced Teaching Techniques', 'Database Optimization', 'VirtualBox', 'SQL Server 2008', 'Linux (RHEL)/UNIX', 'Hyper-V', 'Bootstrap', 'Prosper9', 'Import/Export', 'Selenium', 'Data Entry Software', 'Web Application Maintenance', 'Swift', 'IBM OPM', 'Agile Scrum Methodology', '2005', 'OS Utilities (TOAD', 'Microsoft SQL Server 2016', 'Windows 2010', 'Compliance Management', \"Prism Microsystems' Event Tracker\", 'MailChimp', 'REST API', 'SugarCRM', 'Perfmon', 'CloudFormation', 'OFA', 'Azure Platform', 'Redhat', 'Linux/Unix', 'Citrix Presentation Server & Access Gateway', 'Scheduling', 'COLDFUSION', 'Apple)', 'SSAS', 'Cisco Unified System', 'Microsoft Power BI', 'PostGIS', 'NIST800-53', 'Barcoding Software', 'Objective c (4 years)', 'ETL Processes', 'Project Management Tools', 'Oracle Database', 'SQL Server Integration Services (SSIS)', 'EXPDP/IMPDP', 'Coaching and Mentoring', 'Oracle OEM Grid', 'ADM', 'Red Gate', 'Windows Server', 'MySQL servers', 'Linux (6 years)', 'ServiceNow Ticketing System', 'SECURITY', 'AJAX', 'IPSec', 'MS Word 2016 & 365 - Advanced', 'UNIX (3 years)', 'Sybase Central', 'Unix (6 years)', 'and macOS', 'AWR (Application Workspace Reporter)', 'Sql Server', 'MS SQL Server 2005+', 'REPLICATION', 'Storage Management', 'Enterprise Software', 'Windows Server Administration', 'Star Schema', 'LexisNexis', 'Program Management', 'Partitioning Strategy', 'VMware Configuration', 'AWS', 'Oracle SQL Developer', 'TOAD 10.1', 'Analysis Services', 'ANDROID', '9i', 'OEM (Enterprise Manager)', 'English', 'Report Development', 'FDIC', 'Microsoft SQL Management Studio (SSMS)', 'MS Windows Server 2008', 'Microsoft access (1 year)', 'MS Word', 'Advanced Security', 'Flask', 'CSS (4 years)', 'Oracle SQL*Plus', 'RAC Administration', 'Impdp', 'Veritas Clustering Services', 'Kerberos Authentication', 'React.js', 'Xamax', 'Cryptography', 'ASP.NET', 'Microsoft Outlook', 'Insight Reporting', 'OLTP Systems', 'RHEL-6', 'High-availability Solutions (AlwaysOn)', 'Snapshot)', 'OBI', 'Oracle 9i', 'Oracle 10g', 'Selenium (4 years)', 'SSL certificates', 'Google Analytics', 'Oracle Support Tools', 'Sybase Central Java Security', 'SAP', 'SHELL/BATCH', 'Database Configuration', 'Bash)', 'Tableau', 'Team Leadership', 'Database Administration (SQL Server', 'MySQL Server', 'iOS', 'CSS3', 'SSH', 'Sql Dba', 'First Level PC Support', 'Google Workspace', 'SQL*Loader', 'Programming Languages (Python', 'Microsoft Office Suite', 'Patch Application', 'Cloud Migration', 'Transactional Replication', 'Linux (RHEL/CentOS)', 'Column-level Security)', 'Run Command', 'Data Integrity', 'MariaDB', 'putty', 'Google Vision API', 'Jira/Confluence', 'DHCP', 'Risk Management Framework (RMF)', 'RPM Packaging', 'Basic Chinese (3 years)', 'Use case', 'Data Warehousing', 'MS SQL Server (2000/2005', 'Server 2012', 'AutoCAD', 'Bloomberg Law', 'Microsoft Access', 'Reporting', 'Angular', 'Databases: MSSQL', 'ITIL', 'Databases (2 years)', 'Toad Data Modeler', 'Frame Relay', 'SQL Server 2012', 'Oracle DataGuard', 'LiteSpeed 5.0', 'Network Technologies (TCP/IP', 'Exchange', 'Heuristics', 'Hardware/Systems Design', 'Windows', 'Linux OS', 'Oracle Enterprise Manager', 'Oracle 10G', 'VLANs/LANS/WANS', 'Oracle 11g Administration', 'Hyper-V for virtualization', 'Problem-solving', 'EXPLAIN PLAN', 'Grafana', 'Git', 'Infrastructure automation', 'SQL Server Management Studio', 'snapshot', 'MS PowerPoint', 'Network Security', 'NetBackup', 'Oracle Database Management', 'RHEL', 'Visual Basic', 'Elastic Load Balancer (ELB)', 'VMs', 'Nmap', 'BIDS', 'RDS/Aurora', 'MS SQL Server 2008/2010', 'JCL', 'Patch Management (CPU', 'Security Management', 'Accounting', 'Scheduler', 'Atlassian Confluence', 'and Percona)', 'Virtual Windows Servers', 'Ansible', 'Streams', 'Data Migration', 'IT Service Management (10+ years)', 'Synergy', 'SolarWinds', 'Query Analysis and Optimization', 'Trello', 'Azure', 'Cybersecurity', 'ASMM', 'Project Management', 'Requirements Gathering', 'API Gateway)', 'Sublime Text', 'DBCA', 'MAC', 'Oracle 10g/11gR2', 'Project Planning', 'Mitigation', 'UDP', 'GitLab', 'Enterprise Linux', 'MadLIB', 'Security Best Practices', 'Export/Import', 'Backup strategies', 'Backups', 'Detail Specificity', 'Capacity Planning', 'Jira Service Desk', 'Backup Software', 'Network Infrastructure Setup', 'Autosys', 'ASM (Automatic Storage Management)', 'Quality Assurance', 'Ethernet', 'Lambda', 'Index Rebuild', 'Tissue culture', 'Oracle 12c/11g', 'IT Security Compliance', 'Mac OS X', 'Data Integration and ETL', 'Semi-structured Interviews', 'Import - Export', 'Log Shipping and Replication', 'Linux (1 year)', 'COBOL', '10g', 'SSRS and SSIS (ETL) Packages', 'Backup systems', 'Cross-platform expertise in Windows Server', 'Vagrant', 'Google Drive', 'UNIX shell scripts', 'TSQL Programming', 'Royal TS', 'Data management & governance', 'Matillion ETL', 'Production', 'Database Migration Tools', 'DonorSearch', 'Microsoft SQL Server certifications (2008', 'FRONT-END', 'Oracle)', 'SQL Code and design (stored procedures', 'EOM Grid Control', 'RMAN Backup and Recovery', 'Oracle Products (Certifications)', 'MICROSOFT SQL SERVER', 'Database Management Systems', 'Microsoft Technology Server 2008', 'CRS)', 'Dell Servers', 'Application Tuning', 'Citrix (4 years)', 'VSAMASSIST', 'MS Outlook 2016 & 365 - Advanced', 'DataGuard', 'Business Analysis', 'SQL Server Management Studio (SSMS)', 'Database Backup and Recovery (RMAN)', 'Event Organization', 'Visual Studio 2010', 'RMAN (Recovery Manager)', 'ADP', 'Nist', 'SQL*', '12g', 'Data Analysis', 'Windows (2000', 'Swedish Massage Techniques', 'Blackbaud Sphere', 'Data Integrity Management', 'AWS Big Data Ecosystem', 'Netsmart', 'SQL', 'SSMA', 'Regular Export/Import', 'JAX', 'API Gateway', 'Checkpoint', 'Spyware Removal Tools', 'Oracle APEX', 'MobaxTerm', 'Oracle Export/Import Utilities', 'EVM', '2014)', 'Virtualization (VMware)', 'RedGate', 'Data Migration (Less than 1 year)', 'Microsoft SQL Server (2005', 'SolarWinds Database Performance Monitor', 'Web Inspect', 'PowerShell', 'Winscp', 'MapForce Server', 'Query Tuning', 'WINDOWS', 'Microsoft SQL Server (2008R2', 'DATABASE (2 years)', 'TestRail (4 years)', 'Lotus Notes', 'Teaching and Coaching Ability', 'Remedy', 'Datapump', 'UNIX Operating Systems', 'Native Backup', 'Q-Replication', 'Redgate SQL Monitor', 'PHP (9 years)', 'Data Backup/Recovery', 'TDE Implementation', 'Architecture', 'Ms SQL Server 2008', 'Win 7 & 10', 'Node.js', 'F5 BIG-IP LTM/GTM', 'Report Writing', 'GNU', 'App Scan', 'Microsoft SharePoint 2013', 'System Administration (6 years)', 'Licensing', 'IBM SPSS', 'Oracle Database Administration (2 years)', 'Network Monitoring', 'Email Marketing', 'SRM', 'SCCM', 'Hardware installation/maintenance', 'System Administration', 'Software Development', 'JQuery', 'Remedy System', 'SQL reporting', 'DBNEWID2', 'Symplicity', 'EBS (Enterprise Business Suite)', 'Transact-SQL', 'Cloud/Azure Integration', 'Puppet', 'MS Office Suite', 'Skype', 'Metalogix', 'Materialize', 'SAFe Agile', 'LINUX', 'Linux and Unix System Administration', 'Linux/Unix (RHEL', 'Oracle Dba', 'NAT/PAT', 'Backup and Restore', 'Crystal Report', 'FTP', 'Tech Support', 'PL/SQL Development', 'Scripting', 'Android Studio', 'BOOTSTRAP', 'Variphy', 'SQL Server 2014', 'Disaster Recovery (Physical and Logical Standby)', 'PssDiag', 'CSS', 'VPN)', 'System integration and optimization', 'SolarWinds Orion', 'Data Pump (expdp', 'PyCharm', 'Cross-Functional Collaboration', 'Backup and Recovery', 'Korn Shell Scripting', 'System Upgrades', 'NFS', 'Amazon Aurora', '8', 'Windows 2008', 'Perl Scripting', 'Veritas NetBackup', 'Databases', 'Linux/AIX/Windows Operating Systems', 'Replication (transactional', 'Virtual Desktops', 'Oracle Streams', 'Programming Languages: SQL', 'PostgreSQL (7 years)', 'Zephyr', 'MFA', '2005/2008 R2', 'Content Management Systems (EMC Documentum Content Server)', 'E-Tapestry', 'RemotelyAnywhere', 'SunOS/Solaris', 'Root Cause Analysis', 'expdp/impdp', 'Profiler', 'Visio', 'ERP Systems (Microsoft Dynamic AX 4.0', 'Oracle Certified Professional', 'DBUA', 'Kubernetes', 'Quality Assurance Testing Tools', 'Microsoft Office (over 10 years)', 'VM Configuration', 'RAID', 'ERP System Optimization', 'ORA Error Resolution', 'A/B Testing', 'Toad for Oracle', 'Pardot', 'Ms Access', 'HP BSM Monitoring Tool', 'Adobe Flash/Animate', 'VM creation and deployment', 'MYSQL', 'DBA', 'HTTP', 'IOS', 'Litespeed', 'Oracle BI (OBIEE)', 'Symfony', 'Ubuntu', 'Oracle Database 10g', 'RAC (Real Application Clusters) Management', 'System Integration', 'MongoDB', 'VMware Virtualization', 'High-Availability Cluster Configuration', 'Salesforce Administration', 'Share Point T-SQL', 'BMC Control-M', 'Oracle Data Pump', 'SSRS', 'FedRAMP', 'Oracle (4 years)', 'Diagnostic Equipment', 'PYTHON', 'Maximo', 'Hipchat', 'Disaster Recovery Solutions', 'Community Engagement', 'Unix Shell Scripting (Perl', 'AWS SDK for PHP', 'Visual Basic 6', 'ETL', 'SQL Server 2005', 'Data Optimization', 'Ultraedit', 'Solaris 10', 'WordPress', 'User Research', 'Security Compliance and Auditing', 'SQL Programming', 'Self-starter with problem-solving abilities', 'Mercurial', 'HER', 'Administration', 'Database Security and Compliance', 'Excel (5 years)', 'Backup Strategy Implementation', '.NET', 'Windows XP/8/10', 'SSMS', 'Oracle Enterprise Manager Grid Control (OEM 12c)', 'Backup/Restore Processes', 'Cisco Call Manager', 'RPG Free Form (minimal)', 'views', 'functions', '3D Studio Max', 'AMM', 'PL SQL', 'Blackboard', 'Informix', 'Desktop Support', 'Apple', 'SSRS (SQL Server Reporting Services)', 'Programming', 'Database Partitioning and Cloning', 'Automatic Workload Repository (AWR)', 'NetApp SAN Solutions', 'SSIS for ETL Processes', 'SQL Server Reporting Services (SSRS)', 'Fiix', 'Maximo 7.6', 'WSUS', 'Red Gate SQL Tool', 'Jupyter Notebook', 'Kintera', 'Filing', 'Service Control Manager (SCM)', 'NIST Compliance', 'Oracle (1 year)', 'C/C++', 'Hardware & Software Support', 'Microsoft SQL Server DBA', 'Oracle 11g R1 and 12c DBA', 'ENCRYPTION', 'MSSQL (4 years)', 'RMAN (2 years)', 'SharePoint', 'JBOS', 'COMPUTER TROUBLESHOOTING', 'Legal Research', 'TFS', 'Flow charts', 'DJANGO', 'OEM 13c & 12c', 'Performance Optimization', 'MySQL (1 year)', 'SEA ABSmessage Monitoring', 'Toad (2 years)', 'Data visualization', 'Video Editing', 'Virtualization', 'ADO.NET', 'Query Store', 'Redshift', 'Shell Scripting', 'SQLIOsim', 'Project Management: Microsoft Project', 'Accounting Knowledge', 'Office365', 'Loader', 'Microsoft Visual Studio 2008', 'Remedy Ticketing Tool', 'Linux System Administration', 'Joomla', 'Exadata Administration', 'Shell Scripts', 'Adobe Acrobat', 'Service Desk', 'RIP', 'EOM', 'BigBrother', 'Shell Scripting for Automation', 'VLOOKUP Formulas', 'Windows 2003', 'SQL Design Principles', 'Cloud Assessment and Migration', 'Visual Studio', 'Photshop', 'Microsoft Azure Cloud', 'SONAR G', 'OEM', 'EMC Documentum xPlore', 'ASH (Active Session History)', 'Written and verbal communication', 'Big Brother', 'TOAD', 'Backup', 'ADO', 'PostgreSQL Administration', 'High Availability and Disaster Recovery', 'DBMS_STATS', 'Sybase', 'Aircraft Maintenance Procedures', 'MS OFFICE', 'XP; Windows Server 2003', 'Oracle Golden Gate', 'Contracts', 'TOP', 'Legal Support', 'AWR/ADDM', 'CRS Cluster', 'SQL*Plus', 'PowerPoint', 'Optimization', 'Drupal', 'Backup/Restore Plan Implementation', 'Database Performance Tuning', 'Oracle Virtual Box', 'IBM mainframe', 'SSRS & SSIS', 'SQL Server Administration', 'IBM AIX V6.1', 'RMAN', 'PGAdmin', 'IT Consultancy Services', 'Report Design', 'CheckPoint Firewall', 'VM Virtual Box', 'TensorFlow', 'Akcelerant', 'Data Integration (ETL)', 'Oracle 12c', 'SFTP', 'Photoshop', 'HP Servers', 'ETL Tools', 'Xabbix', 'Flash Recovery Area Configuration', 'Cmms', 'Active Data Guard', 'JD Edwards', 'Migrations', 'ELB', 'Database Administration (10+ years)', 'RDBMS Concepts', 'Kibana', 'Database Maintenance Plan Wizard', 'Database Administration (DBA)', 'Apache Cassandra', 'Sql', 'Tivoli Decision Support', 'API', 'ETL Process Development', 'WebLogic Server', 'Microsoft Word (10+ years)', 'Fastclone', 'Database Performance Monitoring', 'Deployment', 'SQL Server Reporting Services', 'SSIS and SSRS', 'ADGM (Advanced Grid Management)', 'Backup & Recovery (RMAN)', 'Dataguard', 'Firebase', 'Policy Improvements', 'Cisco Sourcefire', 'SDLC Methodologies', 'Database Migration', 'Performance Tuning and Optimization', 'Firewalls', 'Redgate SQL Compare', 'TKPROF (Toolkit for Performance Analysis)', 'Windows Operating System', 'Creative Writing', 'Webpack', 'DB Engine Tuning Advisor', 'DMV', 'Performance Tuning', 'Firewall', 'Opatch (Oracle Patch Manager)', 'Data Interchange (EDI)', 'Vision App', 'Database', 'Help Desk Support', 'Data Security and Compliance', 'Data Security Management', 'RMAN (Oracle Recovery Manager)', 'Replication and high availability solutions', 'Atlassian Products: Confluence', 'Toad', 'Certified ScrumMaster (CSM)', 'Nonprofit Operations', 'Red Hat', 'Salesforce)', 'Database Management (SQL)', 'T-SQL Programming Language', 'Foglight', 'Maintenance', 'HP-UX', 'Tivoli Monitoring', 'DR Solutions Development', 'SQL LiteSpeed', 'Database maintenance', '2003', 'Epic', 'Marketo', 'Disaster Recovery', 'UNIX(AIX)', 'Database Security and Backup/Recovery', 'Power BI', 'Database Design and Optimization', 'Erwin', 'Backup copies (BCP)', 'VersionOne', 'Ableton Live', 'Transaction Replication', 'Index Tuning Wizard', 'Statistics Update', 'Microsoft Stack', 'Python basics', 'Time Management', 'Illustrator', 'Database Management', 'MONyog MySQL Monitoring', 'PMP', 'ASH', 'DBA (1 year)', 'Visual Flow', 'Strategic Planning', 'Tableau)', 'TOAD 10.0', 'Query Troubleshooting & Tuning', 'DBA Tools', 'Team Viewer', 'RHEL 5.2', '2012 R2', 'Citrix XenApp 7.5', 'RAC Configuration and Management', 'SQL SERVER 2012', 'AS400 Administrator', 'Performance Tuning with STATPACK and AWR', 'QS/TS16949 quality auditing', 'Crontab Scheduling', 'HTML5 (9 years)', 'Performance Analysis', 'ETL Processes (SSIS)', 'Microsoft Office Suite (4 years)', 'AWS EC2', 'SQL queries', 'VB6', 'Microsoft OS (Servers 2008 & 2012)', 'High-Load Database Management', 'Animation', 'Load', 'Extract', 'VNC', 'Microsoft Visio', 'Microsoft Certified: Windows Server 2008/2012 R2', 'Agile Methodology', 'Windows 10', 'Cloud Monitoring', 'Help Desk Solutions', 'CodePipeline', 'Communication and Collaboration', 'Fortran', 'Backup and Recovery (RMAN)', 'OEM Grid', 'AWS RDS', 'SQL Developer', 'PowerPoint)', 'MySQL (original', 'Informatica', 'Postman', '2008 R2/2012/2014/2016)', 'Symantec Endpoint Protection', 'Performance tuning', 'WEBPACK', 'HCI', 'Network administration', 'HAWK', 'Log Shipping', 'Process Optimization', 'Vendor Central', 'PL/SQL', 'VMSTAT)', 'Teamwork', '12c)', 'Professional Scrum Master (PSM I)', 'Oracle Grid Control', 'Security Protocols', 'Oracle Server Software', 'SQL (3 years)', 'Active Directory Management', 'TCP/IP', 'Unix', 'Performance Monitoring and Tuning', 'Exchange Server', 'Oracle Administration', 'Juniper SRX and vSRX', 'Security Management (Oracle Database Vault)', 'Microsoft Azure (2 years)', 'BMC Remedy', '2008R2', 'Flash-Back Recovery', 'AIX', 'Database Administration', 'Information Security', 'System Organization', 'Microsoft Office Suite/365', 'Audit', 'Windows 7 and 8', 'MS Office Suite (Word', 'Audio Editing', 'EMLab', 'SCRUM and Kanban methodologies', 'POSTGRESQL', 'FaceTime', 'RMAN for Backup and Recovery', 'System Upgrade and Maintenance', 'LogMiner', 'Performance tuning with AMM and ASMM', 'Linux Administrator', 'Sales', 'VLANs/LANS/WANS (4 years)', 'Windows Operating Systems', 'ERPs', 'Backup & Recovery', 'High-Availability Environments', 'GoldenGate Configuration', 'RAC/ASM Configuration', 'SQL Loader', 'Mongoose', 'Solaris', 'Cooking', 'Nagios XI', 'Azure Virtualization', 'Postgresql', 'Explain Plan', 'AWS System Manager', 'Digital Electronics', 'RTOS', 'RMAN for Backup & Recovery', 'LAN/WAN', 'LXC', 'Windows 7/8/10', 'JAMA Contour', 'Process optimization', 'PLSQL/SQL', 'Amazon Seller Central', 'Best Practices', 'IP Tables', 'IT Support', 'MySQL (4 years)', 'Amazon Advertising', 'Technology savvy', 'OEM 10g', 'Windows XP', 'Flashback Database', 'Solarwinds', 'HTML5', 'Polycom', 'Proficient in Microsoft Office', 'Mathematica', 'Java Programming', 'Writing and Organizational Skills', 'Infrastructure Management', 'Software integration', 'Server Administration', 'OPS Center for Cassandra', 'Solar Winds Network Performance Software', 'SMS', '2016)', 'Salesforce', 'Information Assurance', 'Power Shell', 'SSIS (Integration Services)', 'Active Session History (ASH)', 'TNSNAMES.ORA', 'Cisco Network Devices', 'BIDS/SSDT', 'LISTENER.ORA', 'Automotive testing', 'Heroku', 'Team Foundation Server (TFS 2013)', 'SonarQube', 'Programming Languages (SQL', 'Word', 'Data Migration and Integration', 'MPLS', 'Sun Solaris', 'Atlassian Tools', 'Terraform', 'UX', 'C', 'AWS CloudFormation', 'TKPROF (Toolkit Profiler)', 'Database (DB)', 'Implementation of Snapshot and Transactional Replications', 'Putty', 'Firewall Protection Software', 'Enterprise Monitoring Tools (OEM Grid Control)', 'Export/Import utilities', 'PL/SQL (2 years each)', 'High Availability Solutions', 'Risk analysis', 'DNS', 'High-Availability Solutions', 'Remote Desktop Connection', 'Windows/Linux servers', 'Backup and Recovery Operations', 'CI/CD', 'C (2 years)', 'MySQL DBA', 'Retina scanning software', 'Reporting Software', 'PeopleSoft', 'Other Relevant Tools and Software', 'CentOS', 'Data Transformation Tools', 'HTML 5 (4 years)', 'SSIS', 'McAfee Endpoint Protection', 'Cloud Solutions', 'Data Modeling', 'Oracle 11g/12c', 'VBA', 'SQL Database Administration', 'SQL Server DBA', 'Deadlock Resolution', 'MSSQL', 'Network Monitoring and Maintenance', 'Canvas', 'React JS', 'Documentation', 'VMware (VCP certified)', 'Adobe Audition', 'Thinkfolio', 'iManis Data', 'Microsoft SQL Server 2012', 'Analytical Problem Solving', 'Network Administration', 'QuickBooks', 'Unix Shell Scripting', 'Monitoring Cluster (Active/Active and Active/Passive)', 'RAC (Real Application Clusters)', 'Rally', 'Service Now', 'Database Monitoring', 'UI', 'Rman', 'Perl', 'API Endpoints', 'Cognos', 'Data Presentation', 'Geirs', 'Oracle Databases', 'Openstack', 'Recovery Manager (RMAN)', 'Oracle Databases (10g', 'MS Office', 'SQL (Structured Query Language)', 'Data Validation', 'Netbeans', 'Microsoft Office', 'AIX)', 'Data Transformation Services (DTS)', 'Windows NT', 'Records Management', 'Kanban', 'NetCommunity', 'Troubleshooting', 'Database administration', 'SQL Server 2016', 'Logbook Software', 'GoTo Training', 'Ruby on Rails (4 years)', 'Data analysis', 'CloudWatch', 'Patch Management', 'RedHat', 'Business Intelligence Studio', 'VNC Viewer', 'Splunk', 'REST', 'Front End', 'Oracle Database Administration', 'SQL Tuning Advisor', 'SSL/TLS', 'Performance Tuning (AWR', 'NIST)', 'MySQL Database Management', 'SHARK', 'Statistical Analysis', 'Altru', 'MS PowerPoint 2016 & 365 - Intermediate', 'Symantec', 'MS-SQL', 'Windows 2000 Advanced Server', 'SQL (10+ years)', 'PgAdmin III', 'DBCC', 'DevOps', 'Network and Design', 'EMLS (Enterprise Manager for Oracle)', 'Data Migration and Replication', 'SQL Server Administration/Development', 'High Availability Solutions Certification', 'VPC', 'ERwin', 'System Design', 'BackupExec', 'Golden Gate Configuration', 'Goal oriented', 'Linux', 'JIRA', 'DNS servers', 'Database Migration and Backup', 'DATABASE', 'SQL Queries', 'Windows operating system', 'Jira', 'DBA Monitoring', 'DB2', 'IT Security', 'CommVault', 'Security Patching', 'SAS', 'Tablespace Management', 'RDS', 'Tandberg', 'Adobe Photoshop', 'Incident Handling and Problem Resolution', 'VMware vSphere', 'Java', 'Multichannel Marketing', 'ADDM', 'Marketing Cloud', 'Network security protocols and compliance standards', 'Python', 'Data Pump Exp/Imp', 'Unix/Linux', 'VMWARE', 'Razors Edge', 'Euler Studio', 'SMTP', 'Data Analytics', 'Mailgun API', 'constraints)', 'Metasploit', 'HTML', 'git', 'Reporting Services (SSRS)', 'Relativity', 'Service Desk Assessments', 'Collaboration Tools (Microsoft Office Suite', 'SQL Developer EXP', 'SSIS/SSRS', 'SOAP', 'Governance', 'AutoScaling', 'Linked Servers', 'Customer Service Skills (2 years)', 'Statspack', 'TSPITR', 'Security Tools', 'AWS Certified Cloud Practitioner', 'Objective C', 'Time management', 'AlwaysOn availability replicas', 'PowerBuilder', 'Shareplex Top', 'SQL Profiler', 'SAP - Data Services Designer', 'Communication', 'Orientation', 'MICROSOFT SQL SERVER 2012', 'Project Management Software (Jira)', 'Amazon Web Services (AWS)', 'Report generation', 'CAN', 'BASH Scripting', 'File Management', 'JavaScript', 'RAC (Real Application Clustering)', 'Database Development', 'NETCA', 'Self-Motivation', 'CozyRoc', 'Container Databases (CDBs)', 'Microsoft Office (10+ years)', '2009 & 2012)', 'Datapump Utility', 'Databases (8+ years)', 'TCP', 'databases (1 year)', 'Security', 'Cloud Computing', 'Citrix', 'WinSCP', 'Silverlake', 'Excel)', 'TDE', 'HTML/CSS', 'JIRA for Issue Tracking and Project Management', 'Dbeaver', 'ManageNow', 'Linux and UNIX Systems', 'Eclipse', 'Red Hat Linux', 'Attunity', 'Database Mirroring', 'Visual Studio Express', 'Cross-Platform Data Migration', 'Failover Cluster Implementation', 'DNS Servers', 'AWS Certified Solutions Architect', 'SQL Server Links (LinkedIn)', 'Goal Orientation', 'Data Backup and Recovery', 'Database Administration (SQL)', 'Database Replication', 'VCS', 'Data Modeler', 'Cross-Platform Database Administration', 'Unix Bash', 'Proficient in Photoshop CS6', 'Data Analysis and Reporting', 'MICROSOFT WORD (8 years)', 'Sybase Systems', 'iptables', 'Backup Solutions', 'ERP Systems', 'PL/SQL Developer 12.x/13.x', 'MATLAB', '2008 R2', 'Microsoft Server 2003/2008', 'Performance Monitoring', 'Aircraft Maintenance', 'Legacy software conversion', 'Security: Firewalls', 'Business Intelligence', 'Data Pump (impdp/expdp)', 'Opatch', 'JAVA', 'Fine-Grained Auditing', 'ASM', 'Salesforce.com', 'Toad 12.x/13.x', 'Lean Six Sigma Black Belt', 'Windows PowerShell', 'MDT', 'Business management', 'Microsoft Office Suite (Word', 'ADDM (Advanced Database Diagnostic Monitor)', 'Sage SEI Enterprise reporting', 'PhoneGap', 'Microsoft Tools', 'Datameer', 'LinkedIn', 'Epic systems', 'Linux Shell Scripting', 'ExaData', 'Sql Database', 'Splunk Express', 'Microsoft Project', 'FreePBX/Asterisk', 'Project coordination', 'Database Migration and Optimization', 'MS Access', 'SQL management', 'Automation Scripting', 'Batch Files', 'AWR', 'DB2 Control Center', 'DBNEWID', 'Windows OS', 'Microsoft Outlook Assessments', 'Database Troubleshooting and Repair', 'Reporting Tools', 'Backup and Recovery Solutions', 'PostgreSQL (4 years)', 'Database Analysis', 'Microsoft PowerShell', 'SAFe', 'Physical and Logical Data Modelling', 'Call Center', 'SAN/NAS Infrastructure Configuration', 'CUSTOMER SERVICE', 'SQL Development', 'ACCESS (2 years)', 'Maru/Matchbox', 'SQL Tuning', 'C#', 'Windows Server 2008/2012', 'Tivoli', 'Oracle ODA', 'MCSA: SQL Server', '2012/2014/2016)', 'Pivot Tables', '8 & 10', 'Cerner', 'Extended Events', 'Expdp', 'SQL Developer Work Experience', 'Database Design and Implementation', 'Microprocessors', 'SSRS)', 'System Analysis', 'Agile Methodologies', 'ISO27001', 'Chef', 'EMLab Grid Control', 'Oracle Databases (8i', 'Data Guard Implementation', 'BlackBerry', 'C++ (4 years)', 'Microsoft Excel', 'SQL Scripting', 'and Access Database', 'Medical Manager', 'Sharegate', 'Thinkful Curriculum', 'Constraints', 'PHP', 'CISCO', 'SourceTree', 'ADP Payroll', 'Data Domain', 'Pluggable Databases (PDBs)', 'Account reconciliation', 'AIX and Linux System Administration', 'ArcSight', 'Security Implementations', 'AWR (Automatic Workload Repository)', 'OBJECTIVE C', 'Angular.js', 'WINSCP', 'AddM', 'Microsoft SQL Server (2005-2014)', 'Arcserve', 'IBM Data Studio', 'SharePoint Administration', 'Backup/Recovery', 'Atlassian (Jira)', 'Crystal Reports', 'User and Role Management', 'Enterprise Manager', 'GoldenGate', 'React', 'Networking', 'Powerpoint', 'e-business', 'Jack Henry system', 'Crontab', 'SSSD', 'Idera SQL Diagnostic Manager', 'CLI', 'SQL-Loader', 'Index Tuning', 'Amazon DMS', 'Juniper setup', '10 key', 'OEL', 'TFS (Team Foundation Server)', 'Cloud Formation', 'Personal Injury', 'Windows Server 12/16', 'Database Tools: Oracle Enterprise Manager', 'Netgear', 'SQL*LOADER', 'Adobe', 'ERP System Integration', 'System Customization', 'Networking: TCP/IP', 'Visual Studio for Development and Debugging', 'SharePoint Technologies', 'Quest QDesigner', 'Clustering', '12c', 'Query Performance Tuning', 'QNAP', 'Samba File Sharing', 'GRID CONTROL', 'Data Migration-Export-Import', 'JetBrains IDEs', 'VPN', 'Oracle RDBMS', 'MicroStrategy', 'MONGODB', 'ASP.NET MVC', 'Shell', 'Galera Clustering', 'EC2 Instances', 'Bash', 'SSIS (SQL Server Integration Services)', 'Focus Groups', 'High-Availability Management', 'Google Drive (6 years)', 'Other Reporting Tools', 'PHOTOSHOP', 'Goldengate', 'Visual Paradigm', 'Receptionist', 'Network and SAN Knowledge', 'SQL Query Optimization', 'Index Management', 'ADDM (Alerts for DBDAs)', 'SQL Database Management', 'Enterprise Application Integration (EAI)', 'Database Design Techniques', 'Tortoise SVN', 'Customer Support', 'UNIX/Linux', 'Microsoft Exchange Server 2003', 'SAP-Data Services Designer', 'Stakeholder Engagement', 'Process Improvement', 'Scrum', 'Materialized Views', 'Dreamweaver', 'Data Architecture', 'SQL Server Management', 'Idera', 'DonorBox', 'PHP)', 'Hive', 'SQL Server Profiler', 'RAC', 'CRSCTL', 'Cross-Platform Compatibility', 'JavaScript (4 years)', 'Database Design (OLTP and OLAP)', '10', 'Data pump', 'SVN', 'G-Suite', 'SQL SERVER', 'Problem Solving', 'HASKELL', 'Apache Web Server', 'Import/Export Wizard', 'Fisheye', 'Microsoft SQL Server Administration', 'BI Tools', 'Mysql', 'CI/CD pipelines', 'RDBMS (Oracle', 'Cyber Security', 'Cisco VPN Client', 'Virus Protection Software', 'Database Tuning Advisor', 'PLSQL (10+ years)', 'Forecasting', 'SQL/MySQL', 'Computer Hardware', 'MS-SQL Server', 'Idera Diagnostic Manager', 'tables', 'Stored Procedures', 'SQL (9 years)', 'HTML (9 years)', 'Log Analysis', 'SQL DBA', 'C++', 'Tivoli Storage Manager', 'Oracle 11g', 'CRM', 'HAProxy Load Balancing', 'Azure Cosmos DB', 'Basic SQL (1 year)', 'Continuous Integration', 'Batch and Powershell Scripting', 'Network Monitoring Tools', 'POWERPOINT (8 years)', 'Database Configuration and Maintenance', 'Agile methodologies', 'Perl Scripting for Patch Management', 'Database Migration and Upgradation', 'Visio)', 'VM', 'Transportable Tablespaces', 'NX Client', '2012', 'Manage Now', 'OpenStack', 'Delivery', 'Collabnet Subversion Tool', 'Warehouse Management Systems', 'GoTo Meeting', 'ADRCI', 'Waterfall Methodology', 'Toad Data Pump', 'System Configuration and Support', 'Oracle 10g/11g', 'Windows OS (95+)', 'Microsoft Certified Professional', 'Cloud Computing (AWS', 'Views', 'Database Development (10+ years)', 'Compliance', 'Wireless (Cisco', 'SQL Database', 'HTML/CSS/JS', 'Application Request Handling', 'impdp)', 'PBX', 'Data Replication', 'UNIX/Linux Systems Administration', 'Oracle Support', 'Document revision', 'Transportable Tablespaces Implementation', 'Prometheus', 'Restful APIs', 'BMC Change Manager for DB2', 'MUMPS', 'ETL processes', 'Financial Reporting', 'RegEx', 'SonicWall Firewall and Security Solutions', 'Data Archiving', 'Data migration', 'System Administrator', 'SQL Server Analysis Services (SSAS)', 'Linux basics', 'SQL Trace', 'Docker', 'EXP/IMP Tools', 'VMware', 'Log File Monitoring', 'Cisco', 'PL/SQL Developer', 'WinCC OA', 'Data Integrity Monitoring', 'Golden Gate Replication', 'Android', 'Database migration and design', 'PL/SQL Programming', 'Clarity/HMIS', 'NetSuite', 'Apache Tomcat Web Server', 'Data Analysis Tools (SPSS', 'NAS', 'Splinter', 'DBCA Tools', 'DDoS', 'Query Execution Plan', 'Data Pump', 'Idera Access', 'MacOS', 'Oracle Universal Installer', 'Marketing', 'T-SQL Queries', 'T-SQL Programming', 'SecureCRT', 'S3', 'Microsoft Exchange Server', 'DB2 LUW 9.7', 'User Experience', 'Performance Management (10+ years)', 'Microsoft PowerPoint', 'Partitioning', 'Problem-Solving', 'EDM (Enterprise Data Management)', 'Service Pack/Hotfix', 'RAC Configuration', 'Indexes', 'DBVERIFY', 'SQL Server 2008/R2/2012/2017', 'RedGate SQL Toolbelt', 'DataDog', '18c)', 'Outlook)', 'Commvault', 'SSRS (Reporting Services)', 'CRONTAB', 'Activity Monitor', 'Database Administration (8+ years)', 'Oracle 10g/11g/12c DBA', 'Apache', 'Windows XP/Vista/7/8/10', 'SPSS', 'RAC (Real Application Clusters) Configuration and Management', 'Business Intelligence Tools', 'EBS', 'Oracle Cloud', 'T-Sql', 'Perceptive Content', 'Mainframe', 'Apache Tomcat', 'Database Backup Strategies', 'RAC and ASM Configuration', 'AWS VPC', 'Oracle SQL', 'SCOM', 'Hadoop', 'McAfee EPO Server console', 'Real Applications Cluster (RAC)', 'STP', 'XML', 'Database design', 'SQLITE', 'Python (4 years)', 'RAC (Real Application Cluster)', 'Data Tools', 'Data Pump (expdp/impdp)', 'Data Management', 'Availability Groups', 'Help Desk', 'BMC Software', 'Performance Monitor', 'NX client', 'Transparent Data Encryption (TDE)', 'Access (Less than 1 year)', 'distributed systems (2 years)', 'Oracle Grid Control (12c)', 'ERWIN', 'indexes', 'Financial Analysis', 'BIRT', 'HYSYS', 'Shell Programming', 'Windows Server 2003/2008', 'Security Policies']\n"
     ]
    }
   ],
   "source": [
    "print(all_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1c67e826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\gayat\\AppData\\Local\\Temp\\ipykernel_26028\\3083846053.py:3: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  final_df.to_csv(\"G:\\My Drive\\projects\\data_science\\working\\dataset\\skills_correction.csv\", index = False)\n"
     ]
    }
   ],
   "source": [
    "skills.extend(all_skills)\n",
    "skills = sorted(set(skills))\n",
    "final_df.to_csv(\"G:\\My Drive\\projects\\data_science\\working\\dataset\\skills_correction.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32523c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique skills collected: 1995\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "all_skills = []\n",
    "\n",
    "for i in range(1, 652):\n",
    "    path = f\"G:\\\\My Drive\\\\projects\\\\data_science\\\\working\\\\dataset\\\\second_\\\\resume_{i}.json\"\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        continue\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            resume = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    \n",
    "    if \"skills\" in resume and isinstance(resume[\"skills\"], list):\n",
    "        for s in resume[\"skills\"]:\n",
    "            cleaned = re.sub(r'[\\n\"]', '', s).strip()\n",
    "            split_skills = [skill.strip() for skill in cleaned.split(',') if skill.strip()]\n",
    "            all_skills.extend(split_skills)\n",
    "\n",
    "    elif \"skills\" in resume and isinstance(resume[\"skills\"], dict):\n",
    "        for key in [\"technical_skills\", \"tools_and_software\"]:\n",
    "            if key in resume[\"skills\"]:\n",
    "                for s in resume[\"skills\"][key]:\n",
    "                    cleaned = re.sub(r'[\\n\"]', '', s).strip()\n",
    "                    split_skills = [skill.strip() for skill in cleaned.split(',') if skill.strip()]\n",
    "                    all_skills.extend(split_skills)\n",
    "\n",
    "all_skills = sorted(set(all_skills))\n",
    "\n",
    "print(f\"Total unique skills collected: {len(all_skills)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ca1f3298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995\n"
     ]
    }
   ],
   "source": [
    "print(len(all_skills))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c98c2bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "disciplines = [\n",
    "    'HUMANITIES', 'BUDDHIST STUDIES', 'ARCHITECTURE', 'PHARMACY', 'ENGINEERING',\n",
    "    'YOGA', 'LAW', 'VOCATIONAL COURSES', 'COMMERCE', 'MANAGEMENT', 'EDUCATION',\n",
    "    'AGRICULTURE', 'MULTIMEDIA & ANIMATION', 'COMPUTER APPLICATIONS',\n",
    "    'HOTEL MANAGEMENT & TOURISM', 'ALLIED MEDICAL COURSES', 'DESIGN', 'ARTS',\n",
    "    'OPTOMETRY'\n",
    "]\n",
    "disciplines = [x.lower() for x in disciplines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f3114925",
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = [\n",
    "    'Banking and Finance', 'VLSI', 'Ecology and Environment Studies',\n",
    "    'Civil Engineering (Infrastructure Engineering & Management)',\n",
    "    'Medical Technology', 'Material Science Engineering (MEMS)',\n",
    "    'Mechanical Engineering (Manufactruring Engineering)',\n",
    "    'Hospitality and Hotel Administration (H. and H.A.)', 'Digital Marketing',\n",
    "    'Genetics', 'Bio Technology', 'Manufacturing Engineering',\n",
    "    'Medical Laboratory Technology (MLT)', 'Economics and Finance',\n",
    "    'Computer Science & Electrical Engineering',\n",
    "    'Interior Design with Specialization in Accessories and Furniture Design & User Experience Design',\n",
    "    'Pharmaceutical Analysis', 'Epidemiology under Community Medicine',\n",
    "    'Humanities', 'Drug Regulatory Affairs', 'Environmental Science',\n",
    "    'Transportation and Infrastructure Engineering',\n",
    "    'Civil Engineering (Transportation Engg)',\n",
    "    'Disaster Management And National security', 'Robotics & Automation',\n",
    "    'Hospital Administration', 'International Legal and Security Studies',\n",
    "    'Ayurveda', 'Costume & Fashion Design', 'Applied Psychology',\n",
    "    'Information Security', 'Health Information Management',\n",
    "    'Buddhist Studies, Comparative Religion', 'Systems and Control Engineering',\n",
    "    'Computer science and Engineering with specialization in Full stack',\n",
    "    'Power Electronics and Power System', 'Gaming', 'Society and Culture',\n",
    "    'English', 'Entrepreneurship and Family Business',\n",
    "    'Intensive Care Technology', 'Computing and Electrical Engineering',\n",
    "    'Buddhist Studies', 'Graphics and mutlimedia',\n",
    "    'Transportation & Mobility Design', 'Neuro Electro Physiology (NEP)',\n",
    "    'Electronics Design & Technology', 'Civil and Environmental Engineering',\n",
    "    'Human Anatomy', 'Southeast Asian and Pacific Studies', 'Finance',\n",
    "    'Data Science and Engineering',\n",
    "    '(Environment, Climate Change and Sustainability Studies)',\n",
    "    'Computer and Communication Engineering',\n",
    "    'Electronics and Communication Engineering (Spec. in Communication System)',\n",
    "    'Indian Knowledge System', 'English Language Studies',\n",
    "    'Molecular Biology', 'Agricultural Microbiology',\n",
    "    'International Business', 'Mechatronics', 'Media and Communications',\n",
    "    'Computer Science and Engineering with specialization in Data Science and Artificial Intelligence',\n",
    "    'Geo-Technical and Geo-Environmental Engineering',\n",
    "    'Emergency Medical Tech',\n",
    "    'International Development Practice (TISS-Monash University, Australia)',\n",
    "    'Pali Grammar', 'Advance Diploma in Buddhist Studies', 'Avionics',\n",
    "    'Tourism Management', 'Thermal and Fluid Science',\n",
    "    'Stem Cell Science and Technology', 'Digital System',\n",
    "    'VLSI and Embedded Systems',\n",
    "    'Mechanical Engineering (Spec. in Energy Systems)',\n",
    "    'Floriculture and Landscaping', 'Pharmaceutical Bio-Technology',\n",
    "    'Mathematics & Applied Statistics and Informatics',\n",
    "    'Excavation Engineering', 'Anesthesia',\n",
    "    'Internet of Things and Applications', 'Food Processing Technology & Management',\n",
    "    'Performing Arts', 'Computer Science and Entrepreneurship',\n",
    "    'Foreign Trade', 'Mass Communication', 'Business Analytics-IBM',\n",
    "    'Communication System Engineering',\n",
    "    'Instrumentation and Control Engineering', 'Forensic Odontology',\n",
    "    'Journalism & Mass Communication',\n",
    "    'Computer Science and Applied Mathematics',\n",
    "    'Respiratory Care Technology', 'Pharmaceutical',\n",
    "    'Electronics and Computer Engineering', 'Information Technology',\n",
    "    'Production Engineering', 'Embedded Control System', 'Social Science',\n",
    "    'Electronics and Communication Engineering with specialization in Wireless Communication',\n",
    "    'Marketing and Sales',\n",
    "    'IOT integrated with Foundation Certification in Cloud Technology from Micro Focus (Formerly HPE)',\n",
    "    'Electronics and Communication Engineering (ECE)', 'Architecture',\n",
    "    'Pharmacy Administration',\n",
    "    'Computer Science and Engineering with specialization in DevOps',\n",
    "    'Gender Culture and Development Studies', 'Control and Instrumentation',\n",
    "    'Manufacturing Technology & Automation', 'Embedded Systems Engineering',\n",
    "    'Statistics', 'Organic Chemistry', 'E-Commerce', 'Digital Business',\n",
    "    'Medical Radiation Technology (MRT)',\n",
    "    'Computer Engineering (Cloud Technology and Mobile Application)',\n",
    "    'Animation & Multimedia Technology', 'Mathematics', 'Pharmacy',\n",
    "    'Professional Accounting (PA)', 'Energy and Environmental Engineering',\n",
    "    'Career Guidance',\n",
    "    'Animation with International Moving Image Society, UK (Chartered Body)',\n",
    "    'Psychiatry', 'Public Health Dentistry',\n",
    "    'Accident and Trauma Care Technology', 'Database Systems',\n",
    "    'Industrial Engineering Management (IEM)', 'Arabic',\n",
    "    'Fashion Communication', 'Architectural Design', 'Engineering',\n",
    "    'Foreign Languages & Intercultural Studies',\n",
    "    'Computer Science and Business System', 'INSURANCE AND RISK MANAGEMENT',\n",
    "    'Computer Science and Engineering',\n",
    "    'Remote Sensing and Geographic Information Systems', 'Social Sciences',\n",
    "    'Aquaculture', 'IT for Healthcare',\n",
    "    'Production and Industrial Engineering', 'Multimedia and Animation',\n",
    "    'Art and Peace Studies', 'Medical Lab Technology', 'Applied Mechanics',\n",
    "    'Chemistry, Microbiology, Biotechnology (CMBt)', 'Dairy Technology',\n",
    "    'Chinese Language Studies', 'Accounting and Finance',\n",
    "    'Geotechical Engineering', 'Climate Studies', 'Financial Technology',\n",
    "    'Bachelor of Arts and Bachelor of Laws Honors',\n",
    "    'Hospital & Health System Management',\n",
    "    'Mechanical and Materials Engineering',\n",
    "    'Soil Science and Agricultural Chemistry', 'Upstream', 'Criminal Law',\n",
    "    'General Management', 'Sports', 'Strategic HR',\n",
    "    'Irrigation Water Management', 'Interior and Retail Spaces',\n",
    "    'Financial Management', 'Nursing',\n",
    "    'Computer Science and Biosciences', 'Power Electronics', 'Policy Studies',\n",
    "    'specialization in Artificial Intelligence & Robotics', 'Computer',\n",
    "    'Social Work (Dalit & Tribal Studies and Action)',\n",
    "    'Digital & Mass Media',\n",
    "    'Quantum Science and Technology - International Interdisciplinary Masters Program',\n",
    "    'FOREX MANAGEMENT', 'Medical Biotechnology', 'Medical Statistics',\n",
    "    'Design Engineering', 'Animal Biology and Biotechnology',\n",
    "    'Computer Science and Social Sciences', 'Water Management',\n",
    "    'Interior Design', 'Disaster Management',\n",
    "    'Social Work (Community Organisation & Development Practice)',\n",
    "    'Footwear - Design & Production',\n",
    "    'Mechanical Engineering (spec. in Automotive Engineering)',\n",
    "    'Augmented Reality & Virtual Reality (AR & VR)', 'Microbiology',\n",
    "    'Bachelor of Arts and Bachelor of Laws', 'Graphic Design',\n",
    "    'Bachelor Of Business Administration (International Business)',\n",
    "    'Electronics and Instrumentation Engineering (Instrumentation Engineering)',\n",
    "    'Mechanical with specialization in Cyber Physical Systems',\n",
    "    'Applied Psychology (Clinical & Counseling Practice)',\n",
    "    'Centre for Oceans, Rivers, Atmosphere and Land Sciences Earth System Science and Technology',\n",
    "    'Visual Communication', 'Fashion Technology',\n",
    "    'Social Sciences - Tuljapur Campus',\n",
    "    'Computer Science and Engineering with Specialization in Data Science',\n",
    "    'Yoga', 'Machine Design', 'Food Engineering and Technology',\n",
    "    'Agri Business', 'Film and TV Production', 'Music Vocal',\n",
    "    'Environmental Engineering', 'Electrical Engineering (Power System)',\n",
    "    'Public Administration', 'Sensor System Technology',\n",
    "    'Environment Science', 'Mining machinery engineering',\n",
    "    'Transportation Systems Engg', 'Micro Electronics & VLSI Design',\n",
    "    'Higher Diploma In Pali', 'International Relations',\n",
    "    'Social Work (Criminology & Justice)',\n",
    "    'Oil and Gas Management', 'Environmental Studies',\n",
    "    'Civil Engineering Environment Engineering',\n",
    "    'Computer Science and Business systems (CSBS)',\n",
    "    'Journalism and Communication', 'Computer Science and Technology',\n",
    "    'Electrical Engineering Power and Energy Systems', 'Interaction Design',\n",
    "    'Social Media and Culture (HSS)', 'Corporate Secretaryship',\n",
    "    'Thermal Science',\n",
    "    'Organisation Development, Change and Leadership', 'Basic Sciences',\n",
    "    'Soil and Water Engineering',\n",
    "    'Computer Science and Engineering (Spec. in Information Security)',\n",
    "    'Electronics Engineering',\n",
    "    'Human resource Management & Industrial Relations',\n",
    "    'Fashion and Apparel Design with specialization in Fashion Marketing and User Experience Design',\n",
    "    'Comparative Literature',\n",
    "    'Cloud and Information Security integrated with KPMG Certified Cyber Security Professional',\n",
    "    'Speech Language Pathology', 'Process Metallurgy', 'Robotics',\n",
    "    'Design - Interior & Furniture', 'Finance and Accounting',\n",
    "    'Physical Sciences', 'Events, PR & Corporate Communications',\n",
    "    'Computational Science', 'Digital Forensic and Information Security',\n",
    "    'Electronics and Telecommunication Engineering',\n",
    "    'Human Resource Management & Labour Relations',\n",
    "    'Public Policy and Administration', 'Oral Pathology and Microbiology',\n",
    "    'Human Resource Management and Labour Relations',\n",
    "    'Medical Science and Technology', 'Hindu Studies',\n",
    "    'Applied Linguistics', 'Russian', 'Chemical Engineering',\n",
    "    'Applied Electronics', 'Pediatrics', 'Geology and Geophysics',\n",
    "    'Urban Science & Engineering', 'Space Science and Technologies',\n",
    "    'Energy Trading', 'Logistics',\n",
    "    'Artificial Intelligence & Data Science', 'Music',\n",
    "    'Dyestuff Technology',\n",
    "    'Global Management Accountant Integrated with CIMA, UK + IMA, USA + CMA Australia',\n",
    "    'Social Epidemiology',\n",
    "    'Physics Functional Materials and Devices',\n",
    "    'Disaster Informatics and Geospatial Technologies (DIGIT)',\n",
    "    'Silviculture and Agroforestry', 'Economic Sciences',\n",
    "    'Fashion Design', 'Advanced Manufacturing',\n",
    "    'Information Security integrated with KPMG Advanced Certified Cyber Security Professional**',\n",
    "    'Pharmaceutical Science and Technology', 'Law',\n",
    "    'Electronics and Communication', 'Community Medicine',\n",
    "    'Respiratory Medicine', 'Clinical Psychology',\n",
    "    'ADVERTISING AND MARKETING', 'International Trade and Economic Law',\n",
    "    'Liberal Arts',\n",
    "    'Computer Science and Engineering (Spec. in Big Data Analytics)',\n",
    "    'Health Care Management',\n",
    "    'Production and Industrial Engineering (ME)',\n",
    "    'Electrical Engineering Machine Drives and Power Electronics (MDPE)',\n",
    "    'Plantation,Spices,Medicinaland Aromatic Crops',\n",
    "    'Environment & Occupational Health',\n",
    "    'Finance and Computer Applications', 'Aviation Management',\n",
    "    'Communication',\n",
    "    'Biomedical Engineering - International Interdisciplinary Masters Program',\n",
    "    'Epidemology', 'Medical Genetics and Genomics',\n",
    "    'Food, Nutrition and Dietetics', 'Ancient History Culture & Arch.',\n",
    "    'English and Languages', 'Social Work (Children and Families)',\n",
    "    'Social Work (Livelihoods and Social Entrepreneurship) - Guwahati Campus',\n",
    "    'Agriculture & Food Business', 'Electronics - VLSI Design',\n",
    "    'Data Science and Analytics',\n",
    "    'Association of Chartered Certified Accountants (ACCA)',\n",
    "    'Human Genetics and Molecular Biology',\n",
    "    'Electronics and Communication Engineering with Specialization in Microelectronics and VLSI Systems',\n",
    "    'Computer Science ( Specialization in Cyber Security)',\n",
    "    'Forensic Psychology', 'Optional English, Pschology and Journalism',\n",
    "    'Food Science Technology', 'Design - Graphics',\n",
    "    'Automobile Engineering', 'Entrepreneurship',\n",
    "    'Costume Design and Fashion',\n",
    "    'Electrical and Electronics Engineering (specialization in Power Electronics & Drives)',\n",
    "    'Earth and Environmental Sciences',\n",
    "    'International Accounting and Finance with ACCA UK integration#',\n",
    "    'Environmental Sciences', 'Communication and Signal Processing',\n",
    "    'Advanced Materials and Nanotechnology - International Interdisciplinary Masters Program',\n",
    "    'Physical Education and sports',\n",
    "    'Hotel Management and Catering Technology (BHM and CT)',\n",
    "    'Social Work (Disability Studies & Action)',\n",
    "    'Applied Microbiology', 'Computational Fluid Dynamics',\n",
    "    'Architecture and Regional Planning',\n",
    "    'Cyber security and Digital Forensics', 'Audiology', 'Theatre Arts',\n",
    "    'Film and Television Production',\n",
    "    'Biotechnology and Biochemical Engineering',\n",
    "    'Cyber Forensics and Information Security',\n",
    "    'Water Resources Engineering', 'Robotics and Automation',\n",
    "    'Management Accounting and International Finance',\n",
    "    'Statistics with Computer Applications',\n",
    "    'Electrical Engineering (Power Electronics & Drives)', 'Geology',\n",
    "    'Human Resource Management', 'Logistics and Supply Chain Management',\n",
    "    'Earth Sciences', 'Hospitality And Facilities Management',\n",
    "    'Electronics and Communication Engineering with specialization in Microelectronic systems and Internet of Things',\n",
    "    'Banking, Finance & Insurance Law',\n",
    "    'Social Entrepreneurship and International Business (TISS-Queen Marry University, London)',\n",
    "    'Mechatronics & Automation', 'Chinese',\n",
    "    'Food Science and Nutrition', 'Orthopaedic Technology',\n",
    "    'Accounting and Taxation', 'International Finance',\n",
    "    'TOURISM AND EVENT MANAGEMENT',\n",
    "    'Computer Science and Engineering (Spec. in Cyber Security)', 'Agronomy',\n",
    "    'Orthopedics', 'Food and Nutrition', 'Criminal & Security Laws',\n",
    "    'Electronics and Communication with (spl. In Embedded Systems)',\n",
    "    'Architecture and Planning', 'Power Management',\n",
    "    'Cardiac Care Technology', 'Cancer Biology',\n",
    "    'Computer Science and Engineering (Hons) -Cloud Computing in association with IBM',\n",
    "    'Applied Mathematics', 'Signal Processing', 'Medical Pharmacology',\n",
    "    'Technology Alternatives', 'Health Psychology', 'Zoology',\n",
    "    'Cognitive Sciences', 'Energy Engineering (Spec. in Materials)',\n",
    "    'Petroleum Technology',\n",
    "    'Financial Analysis with KPMG International Accounting Practices',\n",
    "    'Ambedkar Thoughts in National Security',\n",
    "    'Liberal Studies and Management', 'Sports, Nutrition & Management',\n",
    "    'Operations Management', 'Financial Analysis & Services',\n",
    "    'Traffic & Transportation Planning', 'Nano Technology',\n",
    "    'Higher Diploma In Buddhist Studies', 'Space Sciences',\n",
    "    'Analytics and Big Data', 'Animation, Gaming & VFX',\n",
    "    'Investment Management', 'Airport and Aviation Business Management',\n",
    "    'Dr. Babasaheb Ambedkars Thoughts on Indias National Security',\n",
    "    'Catering Science and Hotel Management', 'Communication Engineering',\n",
    "    'Design & Manufacturing', 'Engineering Design',\n",
    "    'Computer Science and Engineering with Major in Artificial Intelligence',\n",
    "    'Anasthesia Techology', 'Pharmacognosy', 'Resources Engineering',\n",
    "    'Interdisciplinary Studies',\n",
    "    'Power Electronics and Industrial Drives', 'Chemistry',\n",
    "    'Education (Elementary)',\n",
    "    'Family Business and Entrepreneurship', 'Applied Geology',\n",
    "    'Computer Science Engineering - E-Commerce', 'Financial Services',\n",
    "    'Social Work (Counseling) - Guwahati Campus',\n",
    "    'ECE - Radar and Communication (R&C)',\n",
    "    'Computer Science and Engineering with Specialization in Artificial Intelligence and Robotics',\n",
    "    'Applied Statistics', 'Health Safety and Environment Engineering',\n",
    "    'Leather Goods and Accessories Design', 'Aerospace Engineering',\n",
    "    'Digital Humanities', 'Science of Intelligence', 'Gas Stream',\n",
    "    'Electrical Engineering with specialisation in Communication & Signal Processing',\n",
    "    'Product & Industrial Design', 'Pharmcology', 'Health Economics',\n",
    "    'Biotechnology (spec. in Genetic Engineering )',\n",
    "    'Healthcare Management', 'Perfusion Technology',\n",
    "    'Finance, Marketing & HR', 'Management Sciences',\n",
    "    'Computer Science and Engineering (Hons) -Big Data & Analytics in association with IBM',\n",
    "    'Computer Science, Stats and Mathematics', 'Geography',\n",
    "    'Physician Assistant', 'Design courses',\n",
    "    'Electric Vehicle Technology', 'Seed Science and Technology',\n",
    "    'Horticulture', 'Medical Microbiology',\n",
    "    'Automation & Robotics Engineering',\n",
    "    'Home Science - Clinical Nutrition and Dietetics',\n",
    "    'Physics, Mathematics, Computer Science', 'French',\n",
    "    'Medical Physiology', 'Respiratory Therapy',\n",
    "    'Ocean Engineering and Naval Architecutre Ocean Engg. and Naval Arch.',\n",
    "    'Plant Breeding and Genetics', 'Plant Nematology',\n",
    "    'Forest Products and Utilization', 'Artificial Intelligence',\n",
    "    'Urban Planning', 'Computer Aided Structural Analysis and Design',\n",
    "    'Electronics Science', 'Dental Mechatronics',\n",
    "    'International Relations and Security Studies (with Language specialization)',\n",
    "    'Labour Law', 'Marine Engineering',\n",
    "    'Mechanical Engineering (Design)', 'Alternative Dispute Resolution',\n",
    "    'Journalism', 'Environmental Science and Engineering',\n",
    "    'Chattrapati Shivaji\\'s Vision & Nation Building',\n",
    "    'Textile Engineering and Management',\n",
    "    'Financial Analysis and Control', 'Software Engineering',\n",
    "    'Food Processing Technology',\n",
    "    'Computer Science and Engineering with specialization in Internet of Things',\n",
    "    'Transportation Design', 'Intellectual Property Law',\n",
    "    'Pharmaceutical Technology', 'Tamil',\n",
    "    'Structural and Construction Engineering',\n",
    "    'Metallurgy Engineering and Materials Science',\n",
    "    'Airlines & Airport Management', 'Food Technology',\n",
    "    'Biological Sciences', 'Polymer Engineering and Technology',\n",
    "    'General Medicine', 'Optometry (OPT)',\n",
    "    'Certified Management Accountants (CMA)',\n",
    "    'Water Policy and Governance',\n",
    "    'Cinema and Television', 'Electronics and Communication Engineering',\n",
    "    'Materials Science and Engineering', 'Visual Arts and Photography',\n",
    "    'Epidemiology', 'Genetics and Plant Breeding', 'Commerce',\n",
    "    'Environment and Water Resource Engineering',\n",
    "    'Solar and Alternative Energy', 'Material Science and Technology',\n",
    "    'Master of Hospital Administration (MHA)',\n",
    "    'Computer Science and Engineering (Spec. in Cloud Computing)',\n",
    "    'Oral Medicine & Radiology', 'Yoga Therapy', 'Applied Geochemistry',\n",
    "    'Banking Financial Services & Insurance (BFSI)',\n",
    "    'Internet of Things', 'Mental health nursing', 'Fine Arts',\n",
    "    'Operation & Anaesthesia Technology', 'Energy Engineering',\n",
    "    'Computer Science and Engineering (Hons)- Information Security in association with IBM',\n",
    "    'Ecosophical Aesthetics', 'Prosthodontics', 'Management',\n",
    "    'Civil Engineering and Structural Engineering', 'Islamic Studies',\n",
    "    'IT Infrastructure', 'Cardio Respiratory Physiotherapy',\n",
    "    'AUDIOLOGY AND SPEECH LANGUAGE PATHOLOGY (BASLP)',\n",
    "    'Economics, Psychology, English, Philosophy, Sociology, History',\n",
    "    'Social Work (Mental Health)', 'Rural Management',\n",
    "    'Bharathanatyam', 'Emerging Technologies in Management',\n",
    "    'Management Studies', 'Tourism and Hospitality Management',\n",
    "    'Computer Science Engineering - Cyber Security and Forensics',\n",
    "    'Strategy and Leadership integrated with US CSCA and Institute of Leadership and Management, UK',\n",
    "    'BioScience and Bio Engineering', 'Surgical Oncology',\n",
    "    'International Finance with CGMA-UK', 'Polymer Engineering',\n",
    "    'Library and Information Science', 'Emergency Medicine',\n",
    "    'Computer Science Engineering - Cloud Computing & Virtualization Technology',\n",
    "    'Civil Engineering Transportation Engineering', 'Applied Physics',\n",
    "    'Construction Engineering and Management', 'Metallurgical Engineering',\n",
    "    'Non- Medical', 'Neurosciences',\n",
    "    'ADVANCED COST AND MANAGAMENT ACCOUNTING', 'Pathology',\n",
    "    'Nuclear Medicine Technology', 'NA', 'Physical Chemistry',\n",
    "    'Industrial Safety Engineering', 'CAD / CAM',\n",
    "    'Computer Science Engineering - Open Source & Open Standards',\n",
    "    'Mechanical Engineering (Spec. Artificial Intelligence & Machine Learning)',\n",
    "    'Plant & Tissue Culture',\n",
    "    'Civil Engineering ( Water Resources / Structural Dynamics and Earthquake / Transportation / Structural / Geotechnical Engg.)',\n",
    "    'Electronics and Electrical Engineering', 'VLSI Design',\n",
    "    'Marketing Management', 'Computer Science & Medical Engineering',\n",
    "    'Civil Engineering Geotechnical Engineering', 'Astronomy',\n",
    "    'Electronics and Instrumentation Engineering',\n",
    "    'Civil Engineering Structural Engineering', 'Psychology',\n",
    "    'Social Work (Livelihoods and Social Entrepreneurship)',\n",
    "    'Medical Anatomy', 'Counter Terrorism Studies',\n",
    "    'IT Business Management', 'Education',\n",
    "    'Actuarial Science',\n",
    "    'Health Safety & Environment Engineering With Disaster Management',\n",
    "    'Information Science and Engineering', 'Spanish',\n",
    "    'Communication and Signal Processing (EE)', 'Biochemical',\n",
    "    'Critical Care Technology',\n",
    "    'Computer Science & Business System in association with TCS',\n",
    "    'Regulatory Affairs', 'Food, Nutrition & Dietetics',\n",
    "    'Dialysis Therapy Technology',\n",
    "    'Metallurgy Engineering and Materials Science with specialisation in Material Science and Engineering',\n",
    "    'Clinical Nutrition and Food Science', 'Petroleum Engineering',\n",
    "    'Pharmaceutical Chemistry and Technology',\n",
    "    'Mechanical Engineering (Thermal)',\n",
    "    'Strategic Finance, Branding & Advertising, Event Management',\n",
    "    'Business Studies', 'Automotive Technology', 'Anthropology',\n",
    "    'Clinical Biochemistry', 'Information and Communication Technology',\n",
    "    'Industrial Pharmacy', 'Elementary Teacher Training',\n",
    "    'Fuel Minerals and Metallurgical Engineering', 'Heat Power Engineering',\n",
    "    'Counselling Psychology', 'Food Science and Technology',\n",
    "    'Electrical and Electronics Engineering',\n",
    "    'Journalism and Mass Communication', 'Medical Physics',\n",
    "    'Culinary Arts', 'Power Electronics and Drives',\n",
    "    'Animation and Graphic Design', 'Echo Cardiography',\n",
    "    'PR and Event Management', 'Computational Social Science',\n",
    "    'Big Data Analytics', 'Food Biotechnology', 'Inorganic Chemistry',\n",
    "    'Womens Studies', 'Data Science', 'Medical Imaging Technology',\n",
    "    'Financial Markets', 'Actuarial Management',\n",
    "    'Information and Cyber security', 'Marketing and Finance', 'Physics',\n",
    "    'Bio Science', 'Analytics',\n",
    "    'Buddhist Sanskrit Language and Literature',\n",
    "    'Two Subject Combination', 'Nutrition and Dietetics', 'Cyber Security',\n",
    "    'Ceramic', 'Chemical and Biochemical Engineering',\n",
    "    'Criminal & Security Law', 'Community Science in Food and Nutrition',\n",
    "    'C', 'Control Systems',\n",
    "    'Oils, Oleochemicals and Surfactants Technology',\n",
    "    'Medical Instrumentation', 'Home Science',\n",
    "    'Geo-exploration and Petroleum Geo-sciences',\n",
    "    'Social Sciences ', 'Oral and Maxillofacial Surgery',\n",
    "    'Electronics and Communication with (spl. In VLSI)',\n",
    "    'Civil Engineering and Transportation Engineering',\n",
    "    'Ocean Engineering and Naval Architecture',\n",
    "    'Civil Engineering Construction Engineering and Management',\n",
    "    'Electronics and Communication Engineering (spl. In Biomedical Engineering)',\n",
    "    'Marketing', 'Electronics & Control System Engineering',\n",
    "    'Molecular Medicine & Stem Cell Technologies', 'Industrial Metallurgy',\n",
    "    'Computer Science and Engineering (Spec. in Internet of Things (IOT))',\n",
    "    'Industrial Engineering & Operation Research',\n",
    "    'Mechanical & Industrial Engineering', 'Public Health',\n",
    "    'Biomedical Signal Processing & Instrumentation',\n",
    "    'Hydraulics and Water Resource Engineering',\n",
    "    'International Banking and Finance', 'Computer Engineering',\n",
    "    'Oil and Gas Marketing', 'Head & Neck Oncology',\n",
    "    'Neural and Cognitive Science',\n",
    "    'Computer Science and Engineering with specialization in Artificial Intelligence and Machine learning',\n",
    "    'Food Technology Research', 'Orthodontics',\n",
    "    'Infrastructure Engineering and Management', 'Industrial Product Design',\n",
    "    'Atmosphere and Ocean Sciences',\n",
    "    'Computer Science Engineering - Business Analytics and Optimization',\n",
    "    'Taxation', 'LLB', 'Agricultural Economics',\n",
    "    'Agricultural Extension and Communication',\n",
    "    'Energy Systems (Focused on Solar Energy)', 'Energy and Environment',\n",
    "    'Logistics & Supply Chain Management (LSCM)',\n",
    "    'Mathematics and Computing', 'Disaster Management',\n",
    "    'Electronics and Instrumentation',\n",
    "    'Agricultural and Food Engineering',\n",
    "    'Electrical Engineering (Electric Vehicles)',\n",
    "    'Computer Science and Engineering (Computer Science & Engg. / Artificial Intelligence)',\n",
    "    'Bio Metrics and Cyber Security', 'AUDITING AND TAXATION',\n",
    "    'Oto-Rhino-Laryngology', 'Green Technology',\n",
    "    'Complex Systems and Dynamics - International Interdisciplinary Masters Program',\n",
    "    'Virology & Immunology', 'Energy', 'Wireless Networks & Applications',\n",
    "    'Automotive Electronics', 'Education',\n",
    "    'Fibres and Textile Processing Technology', 'Rotating Equipment',\n",
    "    'Bioinformatics', 'Structural Engineering', 'Space Physics',\n",
    "    'Computer Science (Artificial Intelligence)',\n",
    "    'Financial Economics', 'Automotive Engineering',\n",
    "    'Energy Systems Engineering',\n",
    "    'Banking, Financial Services and Insurance',\n",
    "    'Intellectual Property Rights', 'Pharmaceutics',\n",
    "    'User Experience & Interaction Design',\n",
    "    'CSE (Internet of Things) in association with IBM', 'Accounting',\n",
    "    'Electrical Engineering (Spec. in VLSI)', 'AUTO MARKETING',\n",
    "    'Sports Medicine',\n",
    "    'Computer Science & Engineering - Machine Learning',\n",
    "    'Defence & Strategic Studies', 'Mathematical Sciences',\n",
    "    'Advertising and Marketing Management', 'Biochemical Engineering',\n",
    "    'Archaeological Sources of Buddhist History',\n",
    "    'Industrial Microbiology', 'Prakrit', 'Energy Laws',\n",
    "    'Ad Film Making',\n",
    "    'Mechanical Engineering with specialization in Thermal Energy Systems (TES)',\n",
    "    'Pharmacognosy & Phytochemistry', 'Applicable Mathematics',\n",
    "    'Tourism & Hospitality', 'Travel and Tourism Management',\n",
    "    'Applied Chemistry', 'Construction Technology and Management',\n",
    "    'Plant Physiology', 'CSE - Cyber Security & Digital Forensics',\n",
    "    'Agriculture with specialisation Plant Breeding & Genetics',\n",
    "    'Computer Science & Engineering - Android Application Development',\n",
    "    'Thermal Engineering', 'German', 'Pharma', 'Travel and Tourism',\n",
    "    'Food Production', 'Respiratory Therapy (RPT)',\n",
    "    'Audio Visual Production', 'Genetic Engineering',\n",
    "    'Political Science', 'Agriculture',\n",
    "    'Media and Cultural Studies',\n",
    "    'Civil - Transportation Engineering', 'Business Management',\n",
    "    'Medical Laboratory Technology',\n",
    "    'Intelligent Communication System',\n",
    "    'Civil Engineering Environmental Engineering and Management',\n",
    "    'Physical Education',\n",
    "    'Mechanical Engineering with specialisation in Mechanical Systems Design',\n",
    "    'Materials Science and Metallurgical Engineering',\n",
    "    'Bachelor of Design Interior', 'Computer Technology',\n",
    "    'Digital Animation', 'Water Resources',\n",
    "    'Water Resources Development', 'Natural Sciences',\n",
    "    'Operation Theatre Technology', 'Logistics & Supply Chain Management',\n",
    "    'Data science and Machine learning', 'WOW (World of work)',\n",
    "    'Defence Analyst and National Security', 'International Law',\n",
    "    'Perfumery and Flavour Technology', 'Logistics Management',\n",
    "    'Master of Business Administration',\n",
    "    'Biotechnology (spec. in Regenerative Medicine )',\n",
    "    'CSE with specialization in Cloud Computing (in collaboration with Virtusa)',\n",
    "    'Forensic Science', 'Medical Genetics',\n",
    "    'Dental Operating Room Assistant', 'CSE (Graphics & Gaming)',\n",
    "    'Finance Management', 'Computer Aided Design',\n",
    "    'Embedded Systems and IoT', 'Mechanical Design', 'E-Learning Technology',\n",
    "    'Electronics and Telecommunications Engineering (RF & Microwave)',\n",
    "    'Cosmetic Science', 'Mechanical Engineering Mechanical systems Design',\n",
    "    'Theatre', 'Autonomous Unmanned Vehicle Technologies',\n",
    "    'Law & Technology', 'Electrical and Computer Engineering',\n",
    "    'Vegetable Science',\n",
    "    'Electronics and Communication Engineering with Artificial Intelligence & Machine Learning',\n",
    "    'Immunology', 'Textile Technology', 'Materials Engineering',\n",
    "    'Computer Applications', 'Banking and Insurance',\n",
    "    'Quantum Information and Computation', 'Anaesthesia Technology',\n",
    "    'Renewable Energy, Physics, Computer Science', 'Biochemistry',\n",
    "    'Civil and Infrastructure Engineering',\n",
    "    'Robotics and Mobility Systems', 'Korean',\n",
    "    'Mechanical Engineering (Spec. in Smart Manufacturing)',\n",
    "    'Media and Cultural Studies', 'Fashion and Textiles',\n",
    "    'Media & Entertainment', 'Fruit Science',\n",
    "    'Electronics & Computer Engineering', 'Public and Social Policy',\n",
    "    'Textile Chemistry', 'Clinical Research', 'Industrial Design',\n",
    "    'MLT (Medical Laboratory Technology)',\n",
    "    'Electronics and Electrical Communication Engineering',\n",
    "    'Data Science - International Interdisciplinary Masters Program',\n",
    "    'Quality Assurance', 'Biomedical Engineering (BME)',\n",
    "    'Tourism and Hospitality Management integrated with Chartered Institute of Hospitality, UK',\n",
    "    'Electrical Engineering with specialisation in VLSI Design and Nanoelectronics',\n",
    "    'Linguistics', 'Engineering Sciences',\n",
    "    'Infrastructure Engineering with Specialization in Energy Engineering',\n",
    "    'Building Management',\n",
    "    'Energy Systems - International Interdisciplinary Masters Program',\n",
    "    'Sericulture', 'Political Science - International Relations',\n",
    "    'Maternal & Child Health',\n",
    "    'Computer Science & Engineering with a Specialization in Blockchain Technology',\n",
    "    'Forensic Sciences', 'Medical Biochemistry',\n",
    "    'International Trade & Investment Law', 'Engineering Physics',\n",
    "    'Applied Geophysics',\n",
    "    'Agriculture with Specialisation in Agronomy', 'CA',\n",
    "    'Access to Justice', 'Constitutional Law',\n",
    "    'Cardiovascular Technology', 'Occupational Therapy',\n",
    "    'Agriculture with Secialisation in Agricultural Extension Education',\n",
    "    'Exercise & Sports Sciences', 'Smart Healthcare',\n",
    "    'Garment Designing', 'Embedded System and Technologies',\n",
    "    'Robotics - International Interdisciplinary Masters Program',\n",
    "    'Business Administration',\n",
    "    'Anesthesia & Operation Theater Technology',\n",
    "    'Library and Information Science',\n",
    "    'Guidance and Counselling', 'Manufacturing Technology',\n",
    "    'Entrepreneurship & International Trade',\n",
    "    'Renewable Energy Engineering', 'Neurological Disorders',\n",
    "    'Physiotherapy', 'Plant Biology and Biotechnology',\n",
    "    'Governance and Public Administration', 'Biology',\n",
    "    'Artificial Intelligence and Data Science', 'Imaging Technology',\n",
    "    'Design - Multimedia', 'Mechanical Engineering',\n",
    "    'B. Design - (Product design / Interior design)',\n",
    "    'Mechanical Engineering (Thermal Engg. / Design and Manufacturing / CAD-CAM & Automation / Materials & Manufacturing Technology / Renewable Energy)',\n",
    "    'Cardiothoracic and Pulmonary Disorders', 'Corporate Laws',\n",
    "    'Public Policy and Governance', 'Fluids and Thermal Engineering',\n",
    "    'Graphics and Digital Media', 'Geo-informatics', 'Sociology',\n",
    "    'Textile Design', 'Clinical Embryology', 'Planning',\n",
    "    'Plant Pathology',\n",
    "    'Digital Film Making with International Moving Image Society, UK (Chartered Body)',\n",
    "    'Molecular Biology and biotechnology', 'Climate Sciences',\n",
    "    'Cyber Physical Systems', 'Neuro Psychology',\n",
    "    'Cellular and Molecular Oncology', 'Persian',\n",
    "    'Mechatronics Engineering', 'Endoscopy Technology',\n",
    "    'Five Years Integrated Cours in Defence and Strategic Studies',\n",
    "    'Mechanical Engineering (Spec. in Electric & Hybrid Vehicles)',\n",
    "    'Information System (IS)', 'Geophysics', 'Hospital Management',\n",
    "    'Bioprocess Technology', 'Agricultural Engineering',\n",
    "    'Health Administration',\n",
    "    'Electronics and Communication Engineering with specialization in Machine Learning and Signal Processing',\n",
    "    'Electronics and Communication Engineering With Specialisation in Embeded System',\n",
    "    'Forestry',\n",
    "    'Coastal and Maritime Security Law and Governance',\n",
    "    'Cyber & India’s National Security',\n",
    "    'VLSI Design and Embedded system', 'Musculoskeletal Disorders',\n",
    "    'Forensic Accounting and Financial Fraud Investigation',\n",
    "    'Data Analytics integrated with Foundation Certification Big Data from Micro Focus (Formerly HPE)',\n",
    "    'Botany', 'Hindi',\n",
    "    'Sociology and Social Anthropology', 'Nematology',\n",
    "    'Computer Science and Engineering (Spec. in Software Engineering)',\n",
    "    'Electronics and Communication Engineering (Spec. in IoT and Sensor)',\n",
    "    'Metallurgical and Material Science Engineering',\n",
    "    'Computer Science and Engineering (Spec. in Artificial Intelligence and Machine Learning)',\n",
    "    'Social Work (Women-Centered Practice)',\n",
    "    'Airlines, Tourism & Hospitality', 'Biotechnology',\n",
    "    'Humanities and Social Sciences',\n",
    "    'Hospital and Health Care Management', 'Transport Engineering',\n",
    "    'Yogic Science', 'European Peace & Security',\n",
    "    'Nano Science and Technology', 'Cyber Security Systems and Networks',\n",
    "    'Sustainable Architecture',\n",
    "    'Computer Science and Engineering (International Campus Diu (IIITV-ICD))',\n",
    "    'Soil and Water Conservation Engineering',\n",
    "    'Electronics and Communication Engineering (Spec. in Electronics Systems Design)',\n",
    "    'Sanskrit', 'Sensors and Internet of Things',\n",
    "    'Mechanical Engineering (with sp. in Machine design)',\n",
    "    'Pipeline Engineering', 'Medical Radio Imaging Technology',\n",
    "    'Electrical Engineering (Control & Industrial Automation / Power & Energy Systems Engg)',\n",
    "    'Farm Machinery and Power Engineering', 'Analytical Chemistry',\n",
    "    'Library & Information Sciences (D. Lib.I.Sc.)',\n",
    "    'European Studies', 'Computer Science (Data Science)',\n",
    "    'Computer Science Engineering - Mobile Application Development',\n",
    "    'Conservative Dentistry', 'Community Development',\n",
    "    'CSE (Artificial Intelligence & Machine Learning) in association with IBM)',\n",
    "    'Chemical, Biological, Radiation, Nuclear & National Security (CBRNNS)',\n",
    "    'Mining Engineering', 'Medicinal and Aromatic Plants',\n",
    "    'Cloud Computing & DevOps', 'Multimedia (Visual Communication)',\n",
    "    'Emergency Medicine Technology', 'Integrated Power System',\n",
    "    'Advertising & Digital Marketing', 'Architectural Assistantship',\n",
    "    'Pharmacology', 'Ophthalmology', 'Heat and Power',\n",
    "    'Painting, Print Making, Sculpture',\n",
    "    'Food Technology with Summit Research Program',\n",
    "    'Computer Science with cognitive systems', 'Radiotherapy',\n",
    "    'Graphics & Communication Design', 'Labour Law',\n",
    "    'Obstetrics and Gynecology', 'Diplomacy, Law and Business',\n",
    "    'Corporate Law', 'Women’s Studies',\n",
    "    'Chinese Buddhism', 'General', 'Hotel Management',\n",
    "    'Industrial Engineering', 'Bio-Informatics',\n",
    "    'Structural Dynamics and Earthquake Engineering',\n",
    "    'Information Security and Cyber Forensics', 'Design', 'Life Sciences',\n",
    "    'Sports Management',\n",
    "    'Bachelor of Business Administration and Bachelor of Laws',\n",
    "    'Sciences', 'Control and Automation',\n",
    "    'Psychology, Sociology, Economics', 'Processing and Food Engineering',\n",
    "    'Energy and Infrastructure Management',\n",
    "    'Metallurgical and Materials Engineering',\n",
    "    'Computer Science and IT',\n",
    "    'Center for Electric Vehicles and Intelligent Transport Systems (CEVITS) with specialization in Electric Vehicle Technology',\n",
    "    'Toxicology', 'Economics', 'Biomedical Engineering',\n",
    "    'Regulatory Policy and Governance',\n",
    "    'Computer Science & Design',\n",
    "    'Excellence in Petroleum Engineering', 'Pharmaceutical Chemistry',\n",
    "    'Business Analytics', 'Arts',\n",
    "    'Social Work (Public Health) ',\n",
    "    'Computer Science Engineering - Oil & Gas Informatics',\n",
    "    'Prosthetic and Orthotic',\n",
    "    'Samkhya-Yoga Systems of Indian Philosophy', 'Design - Gaming',\n",
    "    'Health Sciences',\n",
    "    'Electronics and Communications ( Microelectronics & VLSI Design / Communication & Signal Processing Engg)',\n",
    "    'Bioengineering', 'Retail Management',\n",
    "    'Aeronautical Engineering',\n",
    "    'Artificial Intelligence & Machine Learning (AI&ML)',\n",
    "    'Mechanical Engineering (Manufacturing Process & System)',\n",
    "    'Computational Biology', 'Education Studies', 'Social Work',\n",
    "    'Radiology', 'Health Policy, Economics and Finance',\n",
    "    'radiography', 'Agricultural Meteorology', 'Physiology',\n",
    "    'ECE - Very Large-Scale Integration',\n",
    "    'Metallurgy Engineering and Materials Science with specalisation in Metallurgy Engineering',\n",
    "    'Spices, Plantation, Medicinal and Aromatic Plants', 'Psychology',\n",
    "    'International Accounting',\n",
    "    'Pharmaceutical science and engineering', 'Marathi',\n",
    "    'Computer Science and Engineering (Spec. in Information Technology)',\n",
    "    'Social Work (Rural Development) - Tuljapur Campus',\n",
    "    'Entomology', 'Electrical Engineering & Computer Science',\n",
    "    'Civil Engineering (Structural Engg)',\n",
    "    'Sustainable Development and Management',\n",
    "    'Banking, Insurance & Financial Services',\n",
    "    'Mechanical Engineering - CAD/CAM', 'Pali',\n",
    "    'Industrial Engineering and Management',\n",
    "    'Civil Engineering (sp. in Structural Engineering)',\n",
    "    'Power Systems and Automation', 'Renal Dialysis Technology',\n",
    "    'Zoology Research', 'Communication Design', 'Radio diagnosis',\n",
    "    'Urdu', 'Electronics', 'Man made Textile Technology',\n",
    "    'Petroleum Geosciences', 'Business Process Services (BPS)',\n",
    "    'Pharmacy Practice', 'Digital',\n",
    "    'Computer Science and Engineering (Spec. in Data Analytics)',\n",
    "    'Dermatology, Venereology & Leprosy', 'Sports Physiotherapy',\n",
    "    'M.R.I.T. (Medical Radiology Imaging Technology)',\n",
    "    'Earthquake Science & Engineering', 'Diploma of Pharmacy',\n",
    "    'Industrial and Production',\n",
    "    'Computer Science and Systems Engineering',\n",
    "    'Legal Compliance and Secretarial Practices',\n",
    "    'Three Subject Combination',\n",
    "    'Social Work (Community Organisation & Development Practice) ',\n",
    "    'Computer Science and Engineering with Specialization in Artificial Intelligence',\n",
    "    'Marketing and HR', 'Forest Biology and Tree Improvement',\n",
    "    'Computer Science & Engineering with specialization in Cyber Physical Systems',\n",
    "    'Global Peace, Security and Strategic Studies',\n",
    "    'Chemical Sciences', 'Pharmacology and Toxicology',\n",
    "    'Power System Engineering', 'Supply Chain Management',\n",
    "    'B.Tech (Food Technology) - (MBA)',\n",
    "    'Mechanical Engineering with specialization in Advanced Manufacturing (AM)',\n",
    "    'Molecular Microbiology', 'Smart Manufacturing',\n",
    "    'Mechanical System Design(ME)', 'Multimedia and Communication',\n",
    "    'Computer Science Engineering - Internet of Things and Smart Cities',\n",
    "    'Computer Science Engineering - Mainframe Technology',\n",
    "    'Commerce and Management', 'Environmental Management',\n",
    "    'Power System Engineerig', 'Cyber Security and Cyber Law',\n",
    "    'Data and Computational Science', 'Medicinal Chemistry',\n",
    "    'Buddhist Studies, Philosophy & Comparative Religions',\n",
    "    'Metallurgy and Material Science Engineering',\n",
    "    'Clinical Microbiology',\n",
    "    'Ancient Indian History, Culture and Archealogy',\n",
    "    'Data Analytics',\n",
    "    'Infrastructure Engineering with Specialization in Environmental Engineering',\n",
    "    'Electrical Engineering with specialisation in Signal Processing and Communication',\n",
    "    'Cyber-Physical Systems - International Interdisciplinary Masters Program',\n",
    "    'History', 'Microbiology', 'Cryogenic Engineering',\n",
    "    'Retail & Fashion Merchandise', 'ECE with Internet of Things (IoT)',\n",
    "    'Biomedical Genetics',\n",
    "    'Corporate Accounting with CMA USA integration',\n",
    "    'Computational Sustainability', 'Minimal Access Surgery',\n",
    "    'Business Law', 'Instrumentation and Control Systems',\n",
    "    'Taxation', 'Business Process Management',\n",
    "    'Forensic Medicine', 'Agriculture Sciences',\n",
    "    'Cardiovascular Technology (CVTS)', 'Multimedia',\n",
    "    'Digital & Cyber Forensic',\n",
    "    'Data Science and Artificial Intelligence', 'Dance',\n",
    "    'Media & Entertainment Law', 'Palliative Care Professionals',\n",
    "    'Bio-Medical Engineering', 'Quality Assurance Techniques',\n",
    "    'Artificial Intelligence (Summit Research Program)',\n",
    "    'Electronics and Telecommunications Engineering (Spl-VLSI Design & Embedded System)',\n",
    "    'Agri-Business Management',\n",
    "    'Astronomy, Astrophysics and Space Engineering',\n",
    "    'Energy Technology and Management', 'Development Studies',\n",
    "    'Historical Studies', 'B.Tech (Biotechnology) - (MBA)',\n",
    "    'Health Informatics', 'Computational Neuroscience', 'Science',\n",
    "    'Philosophy', 'Constitutional Law',\n",
    "    'Computational Engineering - International Interdisciplinary Masters Program',\n",
    "    'Industrial Chemistry', 'Computer Science',\n",
    "    'Energy Science and Engineering', 'Data Sciences and Data Analytics',\n",
    "    'Advanced Manufacturing and Design',\n",
    "    'Development Studies',\n",
    "    'Labour Studies & Practices',\n",
    "    'Community Health Physiotherapy', 'Nuclear Science and Technology',\n",
    "    'Social Entrepreneurship', 'Optometry',\n",
    "    'Bachelor of Business Administration and Bachelor of Laws Honors',\n",
    "    'Automotive Design Engineering - Fire & Safety Engineering',\n",
    "    'Dialysis Therapy (DIT)', 'Pedodontics',\n",
    "    'Surface Engineering and Technology', 'Mathematics & Statistics',\n",
    "    'Electrical Engineering Control System Engineering',\n",
    "    'Computer Science and Engineering (Spec. in Information Technology and Entrepreneurship)',\n",
    "    'Computer Science and Engineering (Spec. in Bioinformatics)',\n",
    "    'Civil Engineering', 'Electrical Engineering',\n",
    "    'Biosciences and Biomedical Engineering', 'Thermofluids Engineering',\n",
    "    'Ecology, Environment and Sustainable Development',\n",
    "    'Psychiatric Social Work', 'Photography',\n",
    "    'International Business integrated with Chartered Institute of Marketing, UK',\n",
    "    'Agricultural Statistics'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d66cbf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_courses = pd.DataFrame({\"course\": courses})\n",
    "df_courses.to_csv(r\"G:\\My Drive\\projects\\data_science\\working\\dataset\\courses.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "390fb929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(courses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0b5fef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.DataFrame({\"discipline\": disciplines})\n",
    "df_.to_csv(r\"G:\\My Drive\\projects\\data_science\\working\\dataset\\disciplines.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e322fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [\n",
    "    \"BA\", \"B.A\",\n",
    "    \"BSc\", \"B.Sc\",\n",
    "    \"BCom\", \"B.Com\",\n",
    "    \"BBA\", \"B.B.A\",\n",
    "    \"BCA\", \"B.C.A\",\n",
    "    \"BTech\", \"B.Tech\",\n",
    "    \"BE\", \"B.E\",\n",
    "    \"BPharm\", \"B.Pharm\",\n",
    "    \"BEd\", \"B.Ed\",\n",
    "    \"BDS\", \"B.D.S\",\n",
    "    \"BVSc\", \"B.V.Sc\",\n",
    "    \"BHM\", \"B.H.M\",\n",
    "    \"BFA\", \"B.F.A\",\n",
    "    \"BArch\", \"B.Arch\",\n",
    "    \"BDes\", \"B.Des\",\n",
    "    \"BPEd\", \"B.P.Ed\",\n",
    "    \"BStat\", \"B.Stat\",\n",
    "    \"BSW\", \"B.S.W\",\n",
    "    \"BMS\", \"B.M.S\",\n",
    "    \"BBM\", \"B.B.M\",\n",
    "    \"BHMCT\", \"B.H.M.C.T\",\n",
    "    \"BHMS\", \"B.H.M.S\",\n",
    "    \"BAMS\", \"B.A.M.S\",\n",
    "    \"BUMS\", \"B.U.M.S\",\n",
    "    \"BSMS\", \"B.S.M.S\",\n",
    "    \"BNYS\", \"B.N.Y.S\",\n",
    "    \"BPA\", \"B.P.A\",\n",
    "    \"MA\", \"M.A\",\n",
    "    \"MSc\", \"M.Sc\",\n",
    "    \"MCom\", \"M.Com\",\n",
    "    \"MBA\", \"M.B.A\",\n",
    "    \"MCA\", \"M.C.A\",\n",
    "    \"MTech\", \"M.Tech\",\n",
    "    \"ME\", \"M.E\",\n",
    "    \"MPharm\", \"M.Pharm\",\n",
    "    \"MEd\", \"M.Ed\",\n",
    "    \"MDS\", \"M.D.S\",\n",
    "    \"MVSc\", \"M.V.Sc\",\n",
    "    \"MFA\", \"M.F.A\",\n",
    "    \"MArch\", \"M.Arch\",\n",
    "    \"MDes\", \"M.Des\",\n",
    "    \"MPEd\", \"M.P.Ed\",\n",
    "    \"MStat\", \"M.Stat\",\n",
    "    \"MSW\", \"M.S.W\",\n",
    "    \"MMM\", \"M.M.M\",\n",
    "    \"MPA\", \"M.P.A\",\n",
    "    \"LLM\", \"L.L.M\",\n",
    "    \"PhD\", \"Ph.D\",\n",
    "    \"MPhil\", \"M.Phil\",\n",
    "    \"DSc\", \"D.Sc\",\n",
    "    \"DLitt\", \"D.Litt\",\n",
    "    \"MBBS\", \"M.B.B.S\",\n",
    "    \"MD\", \"M.D\",\n",
    "    \"MS\", \"M.S\",\n",
    "    \"DM\", \"D.M\",\n",
    "    \"MCh\", \"M.Ch\",\n",
    "    \"LLB\", \"L.L.B\",\n",
    "    \"LLM\", \"L.L.M\",\n",
    "    \"Diploma\",\n",
    "    \"PGDiploma\", \"P.G.Diploma\",\n",
    "    \"PGDM\", \"P.G.D.M\",\n",
    "    \"PGDBM\", \"P.G.D.B.M\",\n",
    "    \"ADiploma\", \"A.Diploma\",\n",
    "    \"AdvancedDiploma\", \"Advanced.Diploma\",\n",
    "    \"Certificate\",\n",
    "    \"UGCertificate\",\n",
    "    \"PGCertificate\",\n",
    "    \"PGCertification\", \"P.G.Certification\",\n",
    "    \"CA\", \"C.A\",\n",
    "    \"CFA\", \"C.F.A\",\n",
    "    \"CS\", \"C.S\",\n",
    "    \"ICWA\", \"I.C.W.A\",\n",
    "    \"CPA\", \"C.P.A\",\n",
    "    \"CMA\", \"C.M.A\",\n",
    "    \"ACCA\", \"A.C.C.A\",\n",
    "    \"FRM\", \"F.R.M\",\n",
    "    \"CFP\", \"C.F.P\",\n",
    "    \"GNM\", \"G.N.M\",\n",
    "    \"ANM\", \"A.N.M\",  \n",
    "    \"DPharm\", \"D.Pharm\",\n",
    "    \"BMLT\", \"B.M.L.T\",\n",
    "    \"MLT\", \"M.L.T\",\n",
    "    \"BPT\", \"B.P.T\",\n",
    "    \"MPT\", \"M.P.T\",\n",
    "    \"BOT\", \"B.O.T\",\n",
    "    \"MOT\", \"M.O.T\",\n",
    "    \"BASLP\", \"B.A.S.L.P\",\n",
    "    \"MASLP\", \"M.A.S.L.P\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "56ce1359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\gayat\\AppData\\Local\\Temp\\ipykernel_26028\\3587849202.py:2: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  degree_df.to_csv(\"G:\\My Drive\\projects\\data_science\\working\\dataset\\degrees.csv\", index = False)\n"
     ]
    }
   ],
   "source": [
    "degree_df = pd.DataFrame({\"degree\":degrees})\n",
    "degree_df.to_csv(\"G:\\My Drive\\projects\\data_science\\working\\dataset\\degrees.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "78c786a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\gayat\\anaconda3\\lib\\site-packages (4.35.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\gayat\\anaconda3\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Requirement already satisfied: trio~=0.30.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.6.15 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from selenium) (2025.8.3)\n",
      "Requirement already satisfied: typing_extensions~=4.14.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from selenium) (4.14.1)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from webdriver-manager) (24.1)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.3.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d5605f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error locating the description: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x85a8a3+63283]\n",
      "\tGetHandleVerifier [0x0x85a8e4+63348]\n",
      "\t(No symbol) [0x0x693e43]\n",
      "\t(No symbol) [0x0x6dc8de]\n",
      "\t(No symbol) [0x0x6dcc7b]\n",
      "\t(No symbol) [0x0x724ef2]\n",
      "\t(No symbol) [0x0x701464]\n",
      "\t(No symbol) [0x0x72271a]\n",
      "\t(No symbol) [0x0x701216]\n",
      "\t(No symbol) [0x0x6d0855]\n",
      "\t(No symbol) [0x0x6d16f4]\n",
      "\tGetHandleVerifier [0x0xacbb43+2623955]\n",
      "\tGetHandleVerifier [0x0xac6daa+2604090]\n",
      "\tGetHandleVerifier [0x0x88069a+218410]\n",
      "\tGetHandleVerifier [0x0x870ed8+154984]\n",
      "\tGetHandleVerifier [0x0x87742d+180925]\n",
      "\tGetHandleVerifier [0x0x8622b8+94536]\n",
      "\tGetHandleVerifier [0x0x862442+94930]\n",
      "\tGetHandleVerifier [0x0x84d5ea+9338]\n",
      "\tBaseThreadInitThunk [0x0x77205d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x77d7d6db+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x77d7d661+561]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Setup Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "url = \"https://unstop.com/internships/application-engineering-internship-google-1555601\"\n",
    "driver.get(url)\n",
    "\n",
    "try:\n",
    "    # Wait up to 15 seconds for the description to load\n",
    "    description = WebDriverWait(driver, 15).until(\n",
    "        EC.presence_of_element_located(\n",
    "            (By.XPATH, \"//h2[contains(text(),'About the Internship')]/following-sibling::div\")\n",
    "        )\n",
    "    )\n",
    "    print(description.text)\n",
    "except Exception as e:\n",
    "    print(\"Error locating the description:\", e)\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "51f03180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error locating the description: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x85a8a3+63283]\n",
      "\tGetHandleVerifier [0x0x85a8e4+63348]\n",
      "\t(No symbol) [0x0x693e43]\n",
      "\t(No symbol) [0x0x6dc8de]\n",
      "\t(No symbol) [0x0x6dcc7b]\n",
      "\t(No symbol) [0x0x724ef2]\n",
      "\t(No symbol) [0x0x701464]\n",
      "\t(No symbol) [0x0x72271a]\n",
      "\t(No symbol) [0x0x701216]\n",
      "\t(No symbol) [0x0x6d0855]\n",
      "\t(No symbol) [0x0x6d16f4]\n",
      "\tGetHandleVerifier [0x0xacbb43+2623955]\n",
      "\tGetHandleVerifier [0x0xac6daa+2604090]\n",
      "\tGetHandleVerifier [0x0x88069a+218410]\n",
      "\tGetHandleVerifier [0x0x870ed8+154984]\n",
      "\tGetHandleVerifier [0x0x87742d+180925]\n",
      "\tGetHandleVerifier [0x0x8622b8+94536]\n",
      "\tGetHandleVerifier [0x0x862442+94930]\n",
      "\tGetHandleVerifier [0x0x84d5ea+9338]\n",
      "\tBaseThreadInitThunk [0x0x77205d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x77d7d6db+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x77d7d661+561]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "url = \"https://unstop.com/internships/application-engineering-internship-google-1555601\"\n",
    "driver.get(url)\n",
    "\n",
    "# Scroll down slowly to ensure dynamic content loads\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(5)\n",
    "\n",
    "try:\n",
    "    # Try locating the description container (adjust XPath if needed)\n",
    "    description = WebDriverWait(driver, 15).until(\n",
    "        EC.presence_of_element_located(\n",
    "            (By.XPATH, \"//div[contains(@class,'description')]\")\n",
    "        )\n",
    "    )\n",
    "    print(description.text)\n",
    "except Exception as e:\n",
    "    print(\"Error locating the description:\", e)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8b744a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\gayat\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\gayat\\anaconda3\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "409485af",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Loop through all internship cards\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m card \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minternship_meta\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 20\u001b[0m     title \u001b[38;5;241m=\u001b[39m card\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprofile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     21\u001b[0m     company \u001b[38;5;241m=\u001b[39m card\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlink_display_like_text\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     22\u001b[0m     location \u001b[38;5;241m=\u001b[39m card\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation_link\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Base URL for internships (IT/Software category as example)\n",
    "url = \"https://internshala.com/internships/it-software-internship\"\n",
    "\n",
    "# Send GET request\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.7258.155 Safari/537.36\"\n",
    "}\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# List to store all internships\n",
    "internships = []\n",
    "\n",
    "# Loop through all internship cards\n",
    "for card in soup.find_all(\"div\", class_=\"internship_meta\"):\n",
    "    title = card.find(\"a\", class_=\"profile\").text.strip()\n",
    "    company = card.find(\"a\", class_=\"link_display_like_text\").text.strip()\n",
    "    location = card.find(\"a\", class_=\"location_link\").text.strip()\n",
    "    duration_tag = card.find(\"span\", class_=\"item_body\")\n",
    "    duration = duration_tag.text.strip() if duration_tag else \"\"\n",
    "    stipend_tag = card.find(\"span\", class_=\"stipend\")\n",
    "    stipend = stipend_tag.text.strip() if stipend_tag else \"\"\n",
    "    \n",
    "    # Full description page URL\n",
    "    link_tag = card.find(\"a\", class_=\"profile\")\n",
    "    description = \"\"\n",
    "    if link_tag:\n",
    "        internship_url = \"https://internshala.com\" + link_tag.get(\"href\")\n",
    "        resp = requests.get(internship_url, headers=headers)\n",
    "        soup_desc = BeautifulSoup(resp.content, \"html.parser\")\n",
    "        desc_tag = soup_desc.find(\"div\", id=\"internship_detail_container\")\n",
    "        description = desc_tag.text.strip() if desc_tag else \"\"\n",
    "    \n",
    "    internships.append({\n",
    "        \"Title\": title,\n",
    "        \"Company\": company,\n",
    "        \"Location\": location,\n",
    "        \"Duration\": duration,\n",
    "        \"Stipend\": stipend,\n",
    "        \"Description\": description\n",
    "    })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(internships)\n",
    "df.to_csv(\"internshala_internships.csv\", index=False)\n",
    "print(\"Scraping completed! Saved to internshala_internships.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d57a3d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping completed! Data saved to internshala_internships_multiple_pages.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Base URL for the internship category (example: IT/Software)\n",
    "base_url = \"https://internshala.com/internships/it-software-internship/page-{}\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.7258.155 Safari/537.36\"\n",
    "}\n",
    "\n",
    "internships = []\n",
    "\n",
    "# Number of pages you want to scrape (adjust as needed)\n",
    "NUM_PAGES = 5\n",
    "\n",
    "for page in range(1, NUM_PAGES + 1):\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    url = base_url.format(page)\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Loop through all internship cards\n",
    "    for card in soup.find_all(\"div\", class_=\"individual_internship\"):\n",
    "        # Title and detail page link\n",
    "        title_tag = card.find(\"a\", class_=\"profile\")\n",
    "        title = title_tag.text.strip() if title_tag else \"N/A\"\n",
    "        internship_url = \"https://internshala.com\" + title_tag.get(\"href\") if title_tag else None\n",
    "\n",
    "        # Company\n",
    "        company_tag = card.find(\"a\", class_=\"company_name\")\n",
    "        company = company_tag.text.strip() if company_tag else \"N/A\"\n",
    "\n",
    "        # Location\n",
    "        location_tag = card.find(\"a\", class_=\"location_link\")\n",
    "        location = location_tag.text.strip() if location_tag else \"N/A\"\n",
    "\n",
    "        # Duration\n",
    "        duration_tag = card.find(\"span\", class_=\"item_body\")\n",
    "        duration = duration_tag.text.strip() if duration_tag else \"N/A\"\n",
    "\n",
    "        # Stipend\n",
    "        stipend_tag = card.find(\"span\", class_=\"stipend\")\n",
    "        stipend = stipend_tag.text.strip() if stipend_tag else \"N/A\"\n",
    "\n",
    "        # Full description (fetch from internship detail page)\n",
    "        description = \"\"\n",
    "        if internship_url:\n",
    "            resp = requests.get(internship_url, headers=headers)\n",
    "            soup_desc = BeautifulSoup(resp.content, \"html.parser\")\n",
    "            desc_tag = soup_desc.find(\"div\", id=\"internship_detail_container\")\n",
    "            description = desc_tag.text.strip() if desc_tag else \"N/A\"\n",
    "            time.sleep(1)  # polite delay to avoid overloading the server\n",
    "\n",
    "        internships.append({\n",
    "            \"Title\": title,\n",
    "            \"Company\": company,\n",
    "            \"Location\": location,\n",
    "            \"Duration\": duration,\n",
    "            \"Stipend\": stipend,\n",
    "            \"Description\": description,\n",
    "            \"Link\": internship_url\n",
    "        })\n",
    "\n",
    "# Save all data to CSV\n",
    "df = pd.DataFrame(internships)\n",
    "df.to_csv(\"internshala_internships_multiple_pages.csv\", index=False)\n",
    "print(\"Scraping completed! Data saved to internshala_internships_multiple_pages.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e01dd686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: N/A\n",
      "Company: N/A\n",
      "Requirements:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://internshala.com/internship/detail/work-from-home-part-time-subject-matter-expert-anatomy-and-physiology-internship-at-kunduz-technologies-private-limited1757484960\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.7258.155 Safari/537.36\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Title\n",
    "title = soup.find('h1', class_='title').text.strip() if soup.find('h1', class_='title') else 'N/A'\n",
    "\n",
    "# Company\n",
    "company = soup.find('a', class_='company_name').text.strip() if soup.find('a', class_='company_name') else 'N/A'\n",
    "\n",
    "# Requirements / Who can apply\n",
    "requirements = \"\"\n",
    "who_can_apply_tag = soup.find('div', id='job_highlights_container')\n",
    "if who_can_apply_tag:\n",
    "    # Often requirements are in <li> tags inside this container\n",
    "    requirements_list = who_can_apply_tag.find_all('li')\n",
    "    requirements = \"\\n\".join([li.text.strip() for li in requirements_list]) if requirements_list else who_can_apply_tag.text.strip()\n",
    "\n",
    "print(f\"Title: {title}\")\n",
    "print(f\"Company: {company}\")\n",
    "print(\"Requirements:\")\n",
    "print(requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fa9bed08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Subject Matter Expert (Anatomy And Physiology) - Internship\n",
      "Company: KUNDUZ TECHNOLOGIES PRIVATE LIMITED\n",
      "Requirements:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://internshala.com/internship/detail/work-from-home-part-time-subject-matter-expert-anatomy-and-physiology-internship-at-kunduz-technologies-private-limited1757484960\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.7258.155 Safari/537.36\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Title\n",
    "title_tag = soup.find('h1')\n",
    "title = title_tag.text.strip() if title_tag else 'N/A'\n",
    "\n",
    "# Company\n",
    "company_tag = soup.find('a', class_='link_display_like_text')\n",
    "company = company_tag.text.strip() if company_tag else 'N/A'\n",
    "\n",
    "# Requirements / Skills / Eligibility\n",
    "requirements = \"\"\n",
    "requirements_section = soup.find('div', class_='table-container')  # contains all internship details\n",
    "if requirements_section:\n",
    "    li_tags = requirements_section.find_all('li')\n",
    "    if li_tags:\n",
    "        requirements = \"\\n\".join([li.text.strip() for li in li_tags])\n",
    "    else:\n",
    "        requirements = requirements_section.text.strip()\n",
    "\n",
    "print(f\"Title: {title}\")\n",
    "print(f\"Company: {company}\")\n",
    "print(\"Requirements:\")\n",
    "print(requirements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9635e8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Graphic Design & Fundraising\n",
      "Company: Odisha Development Management Programme (ODMP)\n",
      " 7. Build a strong portfolio with published creative work.paigns, social media, and fundraising..than 4 designs, or creating exceptional high-engagement content. videos, and posters that highlight our initiatives and campaigns on social media.\n",
      "Skills: Creativity, Adobe Photoshop, Canva, Graphic Design\n",
      "==================================================\n",
      "Title: Law/Legal\n",
      "Company: Kridey Foundation\n",
      " 4. Engaging in social work and participating in fundraising drives to support the projects run by the NGO.\n",
      "Skills: MS-Office, MS-Word, English Proficiency (Spoken), English Proficiency (Written)\n",
      "==================================================\n",
      "Title: Content Writing & Fundraising\n",
      "Company: Odisha Development Management Programme (ODMP)\n",
      " 7. Build a strong professional writing portfolio with published content under ODMP campaigns.ndraising strategy.ts, or create exceptional, high-engagement content.ia posts and write-ups that highlight our initiatives and campaigns.\n",
      "Skills: Content Writing, Creative Thinking, English Proficiency (Written), Effective Communication\n",
      "==================================================\n",
      "Title: Volunteering\n",
      "Company: NayePankh Foundation\n",
      " This role is ideal for someone who is passionate and motivated to help develop and support communities. Previous experience in volunteer management is advantageous, but not essential. If you have what it takes, we would love to hear from you.ill also be supporting the organization by raising funds. You will support our organization's efforts to encourage volunteering and contribute to the development of the community. \n",
      "Skills: English Proficiency (Spoken)\n",
      "==================================================\n",
      "Title: Social Sector\n",
      "Company: She Can Foundation\n",
      " This role is ideal for someone who is passionate and motivated to help develop and support communities. Previous experience in volunteer management is advantageous, but not essential. If you think you have what it takes, we would love to hear from you.h people, this is the job for you. As a volunteering intern, you will be given four different tasks for different weeks.\n",
      "Skills: Effective Communication\n",
      "==================================================\n",
      "Title: Influencer Manager\n",
      "Company: Dextren Technologies\n",
      " In order to be eligible for the internship, please add the code 8642 as a response to the first question. UK, the US and the EU.  skills.ionship management for a London based fashion brand. This role requires daily engagement, according to UK-Time, with 10-15 mid to micro fashion and lifestyle influencers, managing campaigns and coordinating logistics with suppliers. The successful candidate will lead the influencer management function with professionalism and strategic thinking, working independently while ensuring smooth operations and brand alignment, working for around 4-6 hours daily. \n",
      "Skills: Negotiation, Time Management, MS-Excel, Effective Communication, Influencer Marketing, Community Management, Management\n",
      "==================================================\n",
      "Title: Product Shoot Photographer\n",
      "Company: Dextren Technologies\n",
      " To be eligible for this role, please add the code: 8642 as a response to the first question.dspip of product photography and editing for our jewelry and handbag collections. This long-term internship requires a commitment of 4 hours daily over 2 months. The role requires creativity, technical expertise, and the ability to deliver high-quality visuals that meet the standards of luxury brands.\n",
      "Skills: Adobe Photoshop, Photography, Image Editing\n",
      "==================================================\n",
      "Title: Tele Fundraising\n",
      "Company: NayePankh Foundation\n",
      " 6. Monitor and report on the progress of telefundraising effortsperlysto promote our cause and make appeals to potential donors. This is a great chance to gain real-world experience in development and fundraising.ation and other development projects in rural India. As a telefundraising intern, you will have the opportunity to play an important role in helping to shape the future of the NayePankh Foundation. \n",
      "Skills: Effective Communication\n",
      "==================================================\n",
      "Title: Fundraising Volunteer\n",
      "Company: Odisha Development Management Programme (ODMP)\n",
      " 5. Maintaining the records of the donorsckers and their families  and work\n",
      "Skills: Effective Communication\n",
      "==================================================\n",
      "Title: Swaraj Fellowship & Social Entrepreneurship\n",
      "Company: Hamari Pahchan NGO\n",
      " Be the voice of change—apply now and start your journey with Hamari Pahchan NGO!killspplicationsworking with changemakers & government bodies to boost their career prospects!\n",
      "Skills: MS-Office, Content Marketing\n",
      "==================================================\n",
      "Title: Sales and Marketing\n",
      "Company: Suvita Hair\n",
      " If you are eager to learn, grow, and make a significant impact in the sales and marketing field, then we want to hear from you! Apply now to join Suvita Hair and take the first step towards a rewarding career in sales and marketing.e opportunity to work closely with our experienced sales and marketing team to gain hands-on experience and develop key skills in sales, marketing strategies, customer support, and client relationship management.\n",
      "Skills: Client Relationship, Sales, Marketing Strategies, Customer Support\n",
      "==================================================\n",
      "Title: Graphic Design\n",
      "Company: IREED Academy India Private Limited\n",
      " 5. Manage multiple projects and deliver high-quality work within deadlines.ners, and presentations.brand guidelines.\n",
      "Skills: Adobe Photoshop, Adobe Illustrator, Video Editing, Canva\n",
      "==================================================\n",
      "Title: Event Anchoring & Presenter\n",
      "Company: Marpu Foundation\n",
      " 5. Build a public portfolio of your hosting videos.le.& Hosting (Amazing for your resume!).ion and leading the show? Marpu Foundation is looking for charismatic achor's to be the face and voice of our volunteer community!\n",
      "Skills: Leadership, Event Management, Time Management, Critical thinking, Problem Solving, Public Speaking, English Proficiency (Spoken)\n",
      "==================================================\n",
      "Title: Event Hosting & Facilitator\n",
      "Company: Marpu Foundation\n",
      " 4. Be a Community Champion: Foster a sense of community during events, encouraging interaction and networking among participants. professional experience.ganized individuals to host our diverse range of events. As an Event Host, you will be the friendly face that welcomes our audience, connects with participants, and ensures every event is a memorable and seamless journey for all.\n",
      "Skills: Leadership, Event Management, Team Management, Problem Solving, Public Relations, Public Speaking, Teamwork\n",
      "==================================================\n",
      "Title: Public Speaking\n",
      "Company: Marpu Foundation\n",
      " 4. Bring your own unique style and personality to the stage.a pro.cited!nd events.re.fit.u earn!.ams, and we need charismatic and energetic Event Anchors to be the face and voice of our events (both online and offline).\n",
      "Skills: Leadership, Event Management, Public Relations, Public Speaking, English Proficiency (Spoken), Anchoring, Effective Communication\n",
      "==================================================\n",
      "Title: Workshop Facilitator & Leadership\n",
      "Company: Marpu Foundation\n",
      " 4. Develop Leadership Skills: Hone your abilities in active listening, critical thinking, and conflict resolution in a live, high-stakes environment.at the forefront of India's sustainable development movement. Our mission is to empower individuals and communities through innovative, empathy-led projects aligned with the UN SDGs, supported by over 12 million volunteers.g action.\n",
      "Skills: Leadership, Event Management, Public Relations, Coordination, Public Speaking, Teamwork, English Proficiency (Spoken)\n",
      "==================================================\n",
      "Title: Social Media Marketing\n",
      "Company: Marpu Foundation\n",
      " If you are a proactive and creative individual with a strong understanding of social media marketing, apply now to be part of our team and make a real impact with the Marpu Foundation!sitive change! As an intern, you will have the opportunity to work closely with our marketing team to create engaging content, grow our online presence, and drive meaningful impact in the community. \n",
      "Skills: Social Media Marketing, Digital Marketing, Facebook Marketing, LinkedIn Marketing, Instagram Marketing\n",
      "==================================================\n",
      "Title: Content Writing\n",
      "Company: Marpu Foundation\n",
      " If you are passionate about writing, have a strong command of the English language, and are eager to learn and grow, we want to hear from you! Join us at Marpu Foundation and make a difference through the power of words. Apply now and take the first step towards a rewarding career in content writing.nonprofit sector.\n",
      "Skills: Content Writing, Creative Writing, English Proficiency (Written)\n",
      "==================================================\n",
      "Title: Community Management\n",
      "Company: CollegeDunia Web Private Limited\n",
      " 1. Work closely with content, marketing, and product teams to use the information effectively on the Portal. campaigns.current college students and gather exclusive, real-time insights about colleges across the country. This role involves directly collaborating with students to source valuable and authentic information — including fee structures, new course launches, infrastructure updates, and campus life visuals such as classroom and hostel images.\n",
      "Skills: Social Media Marketing, Team Management, Coordination, Collaboration\n",
      "==================================================\n",
      "Title: Search Engine Optimization (SEO)\n",
      "Company: Sportskeeda\n",
      " 5. To do Competitive Research for multiple sports sections.ies provided.s in the following ways: sourcing data, cleaning data, and top-level data analysis. Tools: MS Excel, G-Sheets\n",
      "Skills: Google Analytics, Search Engine Marketing (SEM), Google AdWords, Search Engine Optimization (SEO)\n",
      "==================================================\n",
      "Title: Law/Legal\n",
      "Company: S Singh\n",
      " 4. Generate detailed reports and maintain accurate case records and deadlines.st in case management tasks.\n",
      "Skills: Social Media Marketing, MS-Office, Content Writing, MS-Word, MS-Excel, Content Marketing, English Proficiency (Spoken)\n",
      "==================================================\n",
      "Title: Campus Ambassador\n",
      "Company: IIT Delhi- Rendezvous\n",
      " 5. Familiarizing yourself with event details and benefits to encourage participationial media, posters and word of mouth.\n",
      "Skills: Social Media Marketing, Digital Marketing\n",
      "==================================================\n",
      "Title: CRM and Retention Manager\n",
      "Company: Scaler\n",
      " 5. Coordinate with design, copy, and product teams for timely campaign launches\n",
      "Skills: Client Relationship Management (CRM)\n",
      "==================================================\n",
      "Title: Business Development (Sales)\n",
      "Company: Eblogtalk\n",
      " 10. Meet or exceed monthly targets for qualified leads and closed deals.ls and proposals.y.channels.line directories.\n",
      "Skills: Digital Marketing, Email Marketing, Lead Generation, English Proficiency (Spoken), English Proficiency (Written), Conversion Rate Optimization\n",
      "==================================================\n",
      "Title: Content Writing\n",
      "Company: IREED Academy India Private Limited\n",
      " 4. Update existing content to improve readability, engagement, and search rankings.ial media, emails, and other digital platforms.\n",
      "Skills: Content Writing, Blogging, Creative Writing, English Proficiency (Spoken), English Proficiency (Written)\n",
      "==================================================\n",
      "Title: Human Resources (HR)\n",
      "Company: Ghar Pe Shiksha\n",
      " 5. Working on maintaining attendance and pay slips of employees BPO, IT, team managers, etc.l sites like Naukri, Shine, Monster, Times Job, LinkedIn, etc.\n",
      "Skills: MS-Office, MS-Word, MS-Excel, English Proficiency (Spoken), English Proficiency (Written)\n",
      "==================================================\n",
      "Title: Graphic Design\n",
      "Company: Jarurat Care\n",
      " 5. Understanding of printing processes and file preparation standards. Must know how to format files with correct resolution, bleed, and specifications for publishers and platforms.ver design or similar creative work.ript's theme, tone, and target audience into visually compelling covers that attract readers and represent the essence of the book.\n",
      "Skills: Creativity, Adobe Photoshop, CorelDRAW, Adobe Illustrator, Video Editing, Adobe Creative Suite, Time Management\n",
      "==================================================\n",
      "Title: Canva Designing\n",
      "Company: Jarurat Care\n",
      " 6. Proficiency in Canvaof social media platforms and content formatsred)set needsks, posters, etc.)for a creative and detail-oriented Canva Designer Intern to join our team! If you live and breathe Canva and love transforming ideas into eye-catching visuals, this is your playground. As our Canva Design Intern, you’ll be responsible for crafting stunning digital assets that elevate our brand and engage our audience.\n",
      "Skills: Creativity, Adobe Photoshop, Time Management, Typography, Animation, Design Thinking, Layout Design\n",
      "==================================================\n",
      "Title: Front End Development\n",
      "Company: Bhaayo Technologies\n",
      " 7. Asking questions, taking feedback, and improving your coding skills dev toolsok good and work well\n",
      "Skills: HTML, CSS, React, React Native, Figma, Next.js\n",
      "==================================================\n",
      "Title: Artificial Intelligence (AI) Integration In Existing Platform - Shoro AI\n",
      "Company: SchoolWaale\n",
      " 3. Integration of language-based API's for translation.r question answering and semantic search.\n",
      "Skills: Python, Machine Learning, Node.js, React, Artificial intelligence, GraphQL\n",
      "==================================================\n",
      "Title: Law/Legal\n",
      "Company: Udaghosh Social Welfare Society\n",
      " If you have serious expertise in patents and want to work on real-time projects, apply now to join our team as a Patent Law Intern.t knowledge of patents, forms, and filing processes are strictly requested not to apply.\n",
      "Skills: MS-Word, English Proficiency (Spoken), English Proficiency (Written)\n",
      "==================================================\n",
      "Title: Law/Legal\n",
      "Company: Sunschein International Private Limited\n",
      " If you are a proactive and detail-oriented individual with a passion for law and international business, we want to hear from you! Join us at Sunschein International Private Limited and kickstart your legal career today. Apply now!n), MS-Word, and MS-Office skills. As an intern, you will have the opportunity to work closely with our legal team and gain valuable insight into various aspects of international business law.\n",
      "Skills: MS-Office, MS-Word, English Proficiency (Spoken), English Proficiency (Written)\n",
      "==================================================\n",
      "Title: Law/Legal\n",
      "Company: Ratio Ventures LLP\n",
      " 5. Equipment: Your own laptop, reliable internet, and phone.covery.king remotely with structured reporting. and write better.ch week.very timelines.rbitration & Conciliation Act, CPC—Order 37, Commercial Courts Act).nce.\n",
      "Skills: MS-Office, English Proficiency (Spoken), English Proficiency (Written)\n",
      "==================================================\n",
      "Title: Humanities\n",
      "Company: Rural Development Society\n",
      " 5. Contributing ideas and insights to enhance the effectiveness of our programs, drive innovation in rural development, and find donors for the campaign\n",
      "Skills: English Proficiency (Spoken), English Proficiency (Written)\n",
      "==================================================\n",
      "Title: Social Work\n",
      "Company: InAmigos Foundation\n",
      " *For Stipend interns have to raise funds*dicated cause.events.ing a social issue.\n",
      "Skills: Effective Communication\n",
      "==================================================\n",
      "Title: Business Development (Sales)\n",
      "Company: Jankalyan Multipurpose Education Society\n",
      " 5. Identify donors and raise fundon various social media platformse creation of projects for the underprivileged sector\n",
      "Skills: English Proficiency (Spoken)\n",
      "==================================================\n",
      "Title: Digital Marketing\n",
      "Company: InAmigos Foundation\n",
      " Note: Stipend depends upon donations raised by the individual.rovements for future campaigns.ial media marketing to join our team and make a meaningful impact with the InAmigos Foundation!\n",
      "Skills: Social Media Marketing, Search Engine Marketing (SEM), Digital Marketing, English Proficiency (Spoken), English Proficiency (Written)\n",
      "==================================================\n",
      "Title: Graphic Design\n",
      "Company: InAmigos Foundation\n",
      " Note: For the stipend, interns have to raise donationsn of new design ideas and creative conceptsmedia, such as social media posts\n",
      "Skills: English Proficiency (Spoken)\n",
      "==================================================\n",
      "Title: Copywriting\n",
      "Company: InAmigos Foundation\n",
      " Note: For the stipend, interns have to raise donations.ty and reach across digital platforms.tion.ach project.vertisements and NGO websites.\n",
      "Skills: Content Writing, English Proficiency (Spoken), English Proficiency (Written)\n",
      "==================================================\n",
      "Title: Sales and Marketing\n",
      "Company: Al Naya International LLC (Sharjah, United Arab Emirates)\n",
      " 5. Performance-based learning and mentorship.o grow.​en sales and marketing.s major platforms.​sectors.​(Tamil preferred).lients and team members.​ation and digital marketing (Facebook, Instagram, YouTube, LinkedIn).​\n",
      "Skills: Digital Marketing, MS-Excel, Problem Solving, Teamwork, Adaptability, Interpersonal skills, Effective Communication\n",
      "==================================================\n",
      "Title: Virtual Volunteer\n",
      "Company: InAmigos Foundation\n",
      " *For Stipend interns have to raise funds* the organizationty\n",
      "Skills: English Proficiency (Spoken), English Proficiency (Written)\n",
      "==================================================\n",
      "Title: Social Media Marketing\n",
      "Company: InAmigos Foundation\n",
      " Note: Stipend depends on donations raised by the individual.he organization's social media profiles..a marketing to join our team and make a meaningful impact with the InAmigos Foundation!\n",
      "Skills: Social Media Marketing, Digital Marketing, Search Engine Optimization (SEO), Facebook Marketing, English Proficiency (Spoken), English Proficiency (Written), Instagram Marketing\n",
      "==================================================\n",
      "Title: Blog Writing\n",
      "Company: InAmigos Foundation\n",
      " Note: The stipend is based on the donations raised by the intern.iting.he NGO’s focus areas to produce relevant content.ng manner.\n",
      "Skills: English Proficiency (Spoken)\n",
      "==================================================\n",
      "Title: Business Development (Sales)\n",
      "Company: Sherlocks Life\n",
      " We want hustlers, not explorers, If you can sell with energy, aggression, and urgency – you’ll thrive here.tionships and closing deals? Join Sherlocks Life as a Business Development (Sales) intern and gain hands-on experience in a fast-paced, innovative environment. \n",
      "Skills: Business Development, Sales\n",
      "==================================================\n",
      "Title: NGO Professional\n",
      "Company: Conscious Humanity Network Foundation\n",
      " If you are a driven individual with a passion for social change and a desire to learn and grow in a supportive environment, we want to hear from you! Join us in creating a better world for all through the Conscious Humanity Network Foundation. Apply now and be a part of something truly impactful.tion.\n",
      "Skills: Leadership, Google Docs, Teamwork, Administrative Support, Collaboration\n",
      "==================================================\n",
      "Title: Anchoring\n",
      "Company: Cogent Web Services\n",
      " 3. Daily work reportingtask within given timeline  handles \n",
      "Skills: Video Editing, Video Making, Hindi Proficiency (Spoken)\n",
      "==================================================\n",
      "Title: Graphic Design\n",
      "Company: Conscious Humanity Network Foundation\n",
      " 7. Stay up-to-date on industry trends and best practices to continuously improve the effectiveness of our design work..rtunity to use your skills in Adobe Photoshop, Adobe Illustrator, and Color Theory to create impactful designs that support our mission of promoting unity and positive change in our communities. Your creativity and expertise will play a vital role in helping us communicate our message effectively to our audience.\n",
      "Skills: Adobe Photoshop, Adobe Illustrator, Color Theory\n",
      "==================================================\n",
      "Title: Video Editing/Making\n",
      "Company: Conscious Humanity Network Foundation\n",
      " Join us in our mission to inspire, educate, and empower individuals to create positive change in the world through the power of video storytelling. Apply now to make a difference with the Conscious Humanity Network Foundation! impactful projects that promote awareness and positive change in the world. If you are passionate about storytelling and making a difference, we want to hear from you!\n",
      "Skills: Adobe Premiere Pro, Adobe After Effects, DaVinci Resolve\n",
      "==================================================\n",
      "Title: Executive Leader\n",
      "Company: Conscious Humanity Network Foundation\n",
      " 3. The tertiary purpose is to evolve the global non-profit ecosystem.vine tools and wisdom globally.n be removed, leading to mutual waste of time and energy.ams, overseeing meeting schedules, tracking attendance, and interacting with colleagues as needed. Be flexible and open to adopting new and innovative ways of working.\n",
      "Skills: Team Management, Google Docs, Coordination, English Proficiency (Spoken), English Proficiency (Written)\n",
      "==================================================\n",
      "Title: E-commerce Executive\n",
      "Company: Conscious Humanity Network Foundation\n",
      " This internship will provide you with valuable hands-on experience in the world of E-commerce while allowing you to contribute to a meaningful cause. Join us in making a difference today!ld. Your knowledge of E-commerce, Market Analysis, Market research, and Marketing Strategy will be put to the test as you assist in driving the growth of our online platform.\n",
      "Skills: E-commerce, Market Analysis, Marketing Strategy, Market research\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://internshala.com/internships/work-from-home-internships\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "internships = []\n",
    "\n",
    "for card in soup.find_all(\"div\", class_=\"individual_internship_details\"):\n",
    "    # Title\n",
    "    title_tag = card.find_previous(\"a\", class_=\"job-title-href\")\n",
    "    title = title_tag.text.strip() if title_tag else \"N/A\"\n",
    "\n",
    "    # Company\n",
    "    company_tag = card.find_previous(\"p\", class_=\"company-name\")\n",
    "    company = company_tag.text.strip() if company_tag else \"N/A\"\n",
    "\n",
    "    # Requirements / About Job\n",
    "    about_tag = card.find(\"div\", class_=\"about_job\")\n",
    "    requirements = about_tag.text.strip().replace(\"\\n\", \" \") if about_tag else \"N/A\"\n",
    "\n",
    "    # Skills\n",
    "    skills = []\n",
    "    skill_tags = card.find_all(\"div\", class_=\"job_skill\")\n",
    "    for skill in skill_tags:\n",
    "        skills.append(skill.text.strip())\n",
    "\n",
    "    internships.append({\n",
    "        \"Title\": title,\n",
    "        \"Company\": company,\n",
    "        \"Requirements\": requirements,\n",
    "        \"Skills\": skills\n",
    "    })\n",
    "\n",
    "# Example output\n",
    "for i in internships:\n",
    "    print(f\"Title: {i['Title']}\")\n",
    "    print(f\"Company: {i['Company']}\")\n",
    "    print(f\"Requirements: {i['Requirements']}\")\n",
    "    print(f\"Skills: {', '.join(i['Skills'])}\")\n",
    "    print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43dfa497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\gayat\\AppData\\Local\\Temp\\ipykernel_11052\\2276153109.py:6: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  file_path = \"G:\\My Drive\\projects\\data_science\\working\\dataset\\synthetic_internship_postings_2000.csv\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details O\n",
      "EcoPulse O\n",
      "is O\n",
      "seeking O\n",
      "a O\n",
      "passionate O\n",
      "PGCertification O\n",
      "Intern O\n",
      "to O\n",
      "join O\n",
      "our O\n",
      "division O\n",
      ". O\n",
      "This O\n",
      "internship O\n",
      "offers O\n",
      "the O\n",
      "platform O\n",
      "to O\n",
      "contribute O\n",
      "to O\n",
      "meaningful O\n",
      "solutions O\n",
      ", O\n",
      "grow O\n",
      "professionally O\n",
      ", O\n",
      "and O\n",
      "build O\n",
      "a O\n",
      "strong O\n",
      "foundation O\n",
      "for O\n",
      "your O\n",
      "future O\n",
      "career O\n",
      ". O\n",
      "As O\n",
      "a O\n",
      "PGCertification O\n",
      "Intern O\n",
      ", O\n",
      "you O\n",
      "will O\n",
      "contribute O\n",
      "to O\n",
      "ongoing O\n",
      "projects O\n",
      ", O\n",
      "collaborate O\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "# Load your dataset\n",
    "file_path = \"G:\\My Drive\\projects\\data_science\\working\\dataset\\synthetic_internship_postings_2000.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define section-to-entity mappings\n",
    "section_map = {\n",
    "    \"Details\": [\"ORG\", \"ROLE\"],\n",
    "    \"Key Responsibilities\": [\"RESP\"],\n",
    "    \"Skills and Qualifications\": [\"SKILL\"],\n",
    "    \"Experience and Expectations\": [\"DISCIPLINE\", \"COURSE\", \"FILTER\", \"DURATION\", \"STIPEND\"],\n",
    "    \"Perks\": [\"PERK\"]\n",
    "}\n",
    "\n",
    "def label_tokens(text, entity_type):\n",
    "    \"\"\"\n",
    "    Tokenizes text and assigns BIO labels for given entity type.\n",
    "    \"\"\"\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    labels = []\n",
    "    first = True\n",
    "    for tok in tokens:\n",
    "        if tok.isalnum():  # words/numbers\n",
    "            if first:\n",
    "                labels.append((tok, f\"B-{entity_type}\"))\n",
    "                first = False\n",
    "            else:\n",
    "                labels.append((tok, f\"I-{entity_type}\"))\n",
    "        else:  # punctuation etc.\n",
    "            labels.append((tok, \"O\"))\n",
    "    return labels\n",
    "\n",
    "def encode_posting(posting):\n",
    "    \"\"\"\n",
    "    Splits a posting into sections, applies labeling, and returns BIO tokens.\n",
    "    \"\"\"\n",
    "    bio_tags = []\n",
    "    \n",
    "    # Split by headers like \"Key Responsibilities:\" etc.\n",
    "    sections = re.split(r'([A-Za-z ]+:)', posting)\n",
    "    \n",
    "    current_section = None\n",
    "    for part in sections:\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "        # If it's a section header\n",
    "        if part.rstrip(\":\") in section_map:\n",
    "            current_section = part.rstrip(\":\")\n",
    "        else:\n",
    "            # If it’s text under a section\n",
    "            if current_section:\n",
    "                for ent in section_map[current_section]:\n",
    "                    labeled = label_tokens(part, ent)\n",
    "                    bio_tags.extend(labeled)\n",
    "            else:\n",
    "                tokens = wordpunct_tokenize(part)\n",
    "                bio_tags.extend([(tok, \"O\") for tok in tokens])\n",
    "    \n",
    "    return bio_tags\n",
    "\n",
    "# Demo: encode first posting\n",
    "sample_posting = df[\"Internship Posting\"].iloc[0]\n",
    "encoded = encode_posting(sample_posting)\n",
    "\n",
    "# Preview first 50 tokens with labels\n",
    "for tok, lbl in encoded[:50]:\n",
    "    print(tok, lbl)\n",
    "\n",
    "# Save in CoNLL format\n",
    "with open(\"internship_postings_bio.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for posting in df[\"Internship Posting\"]:\n",
    "        encoded_post = encode_posting(posting)\n",
    "        for tok, lbl in encoded_post:\n",
    "            f.write(f\"{tok} {lbl}\\n\")\n",
    "        f.write(\"\\n\")  # blank line between postings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05d55883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import random\n",
    "\n",
    "def read_bio_file(file_path):\n",
    "    tokens, labels = [], []\n",
    "    all_tokens, all_labels = [], []\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:  # end of one sample\n",
    "                if tokens:\n",
    "                    all_tokens.append(tokens)\n",
    "                    all_labels.append(labels)\n",
    "                    tokens, labels = [], []\n",
    "            else:\n",
    "                try:\n",
    "                    tok, tag = line.split()\n",
    "                except ValueError:\n",
    "                    continue  # skip malformed lines\n",
    "                tokens.append(tok)\n",
    "                labels.append(tag)\n",
    "    \n",
    "    return {\"tokens\": all_tokens, \"ner_tags\": all_labels}\n",
    "\n",
    "# Load dataset\n",
    "data = read_bio_file(\"internship_postings_bio.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "618b3e4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Unique labels\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m unique_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(tag \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m seq))\n\u001b[0;32m      3\u001b[0m label2id \u001b[38;5;241m=\u001b[39m {label: i \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(unique_labels)}\n\u001b[0;32m      4\u001b[0m id2label \u001b[38;5;241m=\u001b[39m {i: label \u001b[38;5;28;01mfor\u001b[39;00m label, i \u001b[38;5;129;01min\u001b[39;00m label2id\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Unique labels\n",
    "unique_labels = sorted(set(tag for seq in data[\"ner_tags\"] for tag in seq))\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# Map tags to IDs\n",
    "def encode_labels(example):\n",
    "    return {\"ner_tags\": [label2id[tag] for tag in example[\"ner_tags\"]]}\n",
    "\n",
    "# Shuffle + split\n",
    "combined = list(zip(data[\"tokens\"], data[\"ner_tags\"]))\n",
    "random.shuffle(combined)\n",
    "split_idx = int(len(combined) * 0.8)\n",
    "train_data, val_data = combined[:split_idx], combined[split_idx:]\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"tokens\": [t for t, l in train_data], \"ner_tags\": [l for t, l in train_data]})\n",
    "val_dataset   = Dataset.from_dict({\"tokens\": [t for t, l in val_data], \"ner_tags\": [l for t, l in val_data]})\n",
    "\n",
    "train_dataset = train_dataset.map(encode_labels)\n",
    "val_dataset   = val_dataset.map(encode_labels)\n",
    "\n",
    "datasets = DatasetDict({\"train\": train_dataset, \"validation\": val_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f6f09ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting oliveNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading olive-0.2.11-py3-none-any.whl.metadata (346 bytes)\n",
      "Requirement already satisfied: requests in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from olive) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from olive) (0.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from requests->olive) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from requests->olive) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from requests->olive) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from requests->olive) (2025.8.3)\n",
      "Downloading olive-0.2.11-py3-none-any.whl (3.9 kB)\n",
      "Installing collected packages: olive\n",
      "Successfully installed olive-0.2.11\n"
     ]
    }
   ],
   "source": [
    "%pip install olive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b61d1f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93351712d9d749e8ae5a093a9b243a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59269f0ac16b477983bedec68a5c6edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4308995481c641d1a2c8ad97123cc0e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24fe27d488bc4dedb5a71e1a117d11ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/286M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a68775706b43119ff3ee85f0ed39b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/286M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_checkpoint = \"microsoft/deberta-v3-small\"\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(unique_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9f9c2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb4f3b2b6ed4983b7941aeaad898fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m         labels\u001b[38;5;241m.\u001b[39mappend(tag)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m: labels}\n\u001b[1;32m---> 20\u001b[0m dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mmap(parse_conll)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Build label mapping\u001b[39;00m\n\u001b[0;32m     23\u001b[0m unique_tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(tag \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m d)\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\dataset_dict.py:902\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    899\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m    901\u001b[0m     {\n\u001b[1;32m--> 902\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m    903\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[0;32m    904\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[0;32m    905\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[0;32m    906\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[0;32m    907\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[0;32m    908\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m    909\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[0;32m    910\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[0;32m    911\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[0;32m    912\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[0;32m    913\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[0;32m    914\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[0;32m    915\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m    916\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[0;32m    917\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[0;32m    918\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[0;32m    919\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[0;32m    920\u001b[0m         )\n\u001b[0;32m    921\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    922\u001b[0m     }\n\u001b[0;32m    923\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    555\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    560\u001b[0m }\n\u001b[0;32m    561\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 562\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    563\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:3079\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3074\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3075\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3076\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3077\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3078\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3079\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3080\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3081\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:3495\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[0;32m   3494\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 3495\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m iter_outputs(shard_iterable):\n\u001b[0;32m   3496\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[0;32m   3497\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:3469\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.iter_outputs\u001b[1;34m(shard_iterable)\u001b[0m\n\u001b[0;32m   3467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3468\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[1;32m-> 3469\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m i, apply_function(example, i, offset\u001b[38;5;241m=\u001b[39moffset)\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:3392\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function\u001b[1;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[0;32m   3390\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[0;32m   3391\u001b[0m inputs, fn_args, additional_args, fn_kwargs \u001b[38;5;241m=\u001b[39m prepare_inputs(pa_inputs, indices, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[1;32m-> 3392\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39madditional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[0;32m   3393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m, in \u001b[0;36mparse_conll\u001b[1;34m(example)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_conll\u001b[39m(example):\n\u001b[0;32m     11\u001b[0m     tokens, labels \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstrip():\n\u001b[0;32m     14\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\formatting\\formatting.py:280\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m--> 280\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key]\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format:\n\u001b[0;32m    282\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, ClassLabel, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# -------------------------------\n",
    "def parse_conll(example):\n",
    "    tokens, labels = [], []\n",
    "    for line in example[\"text\"].split(\"\\n\"):\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        word, tag = line.split()\n",
    "        tokens.append(word)\n",
    "        labels.append(tag)\n",
    "    return {\"tokens\": tokens, \"ner_tags\": labels}\n",
    "\n",
    "dataset = datasets.map(parse_conll)\n",
    "\n",
    "# Build label mapping\n",
    "unique_tags = set(tag for d in dataset[\"train\"][\"ner_tags\"] for tag in d)\n",
    "unique_tags = sorted(unique_tags)\n",
    "label2id = {tag: i for i, tag in enumerate(unique_tags)}\n",
    "id2label = {i: tag for tag, i in label2id.items()}\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load Microsoft DeBERTa model + tokenizer\n",
    "# -------------------------------\n",
    "model_checkpoint = \"microsoft/deberta-v3-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(unique_tags),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Tokenize + align labels\n",
    "# -------------------------------\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids, previous_word_id = [], None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)  # ignore\n",
    "            elif word_id != previous_word_id:\n",
    "                label_ids.append(label2id[label[word_id]])\n",
    "            else:\n",
    "                # Same word → I- tag\n",
    "                label_ids.append(label2id[label[word_id]] if label[word_id].startswith(\"I-\") or label[word_id].startswith(\"B-\") else -100)\n",
    "            previous_word_id = word_id\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Training setup\n",
    "# -------------------------------\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [\n",
    "        [id2label[l] for l in label if l != -100]\n",
    "        for label in labels\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner-deberta\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Train\n",
    "# -------------------------------\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd079a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from evaluate) (3.3.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from evaluate) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from evaluate) (0.31.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from seqeval) (1.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.10.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2025.8.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16183 sha256=d1149969f905eb7f983a7259ba11a6a3f617b602fd48806b397bfb86656cfd30\n",
      "  Stored in directory: c:\\users\\gayat\\appdata\\local\\pip\\cache\\wheels\\5f\\b8\\73\\0b2c1a76b701a677653dd79ece07cfabd7457989dbfbdcd8d7\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval, evaluate\n",
      "Successfully installed evaluate-0.4.5 seqeval-1.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install evaluate seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "130e726d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5238b9927441c787367f57b8f5883b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0309ec88cb7b4a1a9b9920dc3b1d3dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4880b582415f440b98296525d742602c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Value' object has no attribute 'names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 15\u001b[0m\n\u001b[0;32m     10\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_checkpoint)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 2. Tokenize datasets\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m label_list \u001b[38;5;241m=\u001b[39m datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfeatures[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mnames\n\u001b[0;32m     16\u001b[0m label_to_id \u001b[38;5;241m=\u001b[39m {l: i \u001b[38;5;28;01mfor\u001b[39;00m i, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(label_list)}\n\u001b[0;32m     17\u001b[0m id_to_label \u001b[38;5;241m=\u001b[39m {i: l \u001b[38;5;28;01mfor\u001b[39;00m i, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(label_list)}\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Value' object has no attribute 'names'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "from datasets import DatasetDict\n",
    "import numpy as np\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load pretrained Microsoft model + tokenizer\n",
    "# -------------------------------\n",
    "model_checkpoint = \"microsoft/deberta-v3-base\"   # You can switch to large if you want\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Tokenize datasets\n",
    "# -------------------------------\n",
    "label_list = datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "id_to_label = {i: l for i, l in enumerate(label_list)}\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_id != previous_word:\n",
    "                label_ids.append(label_to_id[label[word_id]])\n",
    "            else:\n",
    "                # Same word, assign I- tag or -100 depending on scheme\n",
    "                label_ids.append(label_to_id[label[word_id]])\n",
    "            previous_word = word_id\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load model\n",
    "# -------------------------------\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Define metrics\n",
    "# -------------------------------\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [\n",
    "        [id_to_label[l] for l in label if l != -100]\n",
    "        for label in labels\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [id_to_label[p] for (p, l) in zip(pred, label) if l != -100]\n",
    "        for pred, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Training setup\n",
    "# -------------------------------\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner-microsoft\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Train\n",
    "# -------------------------------\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67a47e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610b09a1ad7446f5ad891666e2f457a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0d53d17ee64aa1bf6bef91f77592c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbbb2c19641647adb7917745fea79426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "16",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m     tokenized_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m labels_batch\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_inputs\n\u001b[1;32m---> 57\u001b[0m tokenized_datasets \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mmap(tokenize_and_align_labels, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# 4. Load model\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m     62\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     63\u001b[0m     model_checkpoint,\n\u001b[0;32m     64\u001b[0m     num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(label_list),\n\u001b[0;32m     65\u001b[0m     id2label\u001b[38;5;241m=\u001b[39mid_to_label,\n\u001b[0;32m     66\u001b[0m     label2id\u001b[38;5;241m=\u001b[39mlabel_to_id\n\u001b[0;32m     67\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\dataset_dict.py:902\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    899\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m    901\u001b[0m     {\n\u001b[1;32m--> 902\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m    903\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[0;32m    904\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[0;32m    905\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[0;32m    906\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[0;32m    907\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[0;32m    908\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m    909\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[0;32m    910\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[0;32m    911\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[0;32m    912\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[0;32m    913\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[0;32m    914\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[0;32m    915\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m    916\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[0;32m    917\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[0;32m    918\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[0;32m    919\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[0;32m    920\u001b[0m         )\n\u001b[0;32m    921\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    922\u001b[0m     }\n\u001b[0;32m    923\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    555\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    560\u001b[0m }\n\u001b[0;32m    561\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 562\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    563\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:3079\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3074\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3075\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3076\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3077\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3078\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3079\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3080\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3081\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:3519\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3518\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 3519\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m iter_outputs(shard_iterable):\n\u001b[0;32m   3520\u001b[0m         num_examples_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(i)\n\u001b[0;32m   3521\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m update_data:\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:3469\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.iter_outputs\u001b[1;34m(shard_iterable)\u001b[0m\n\u001b[0;32m   3467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3468\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[1;32m-> 3469\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m i, apply_function(example, i, offset\u001b[38;5;241m=\u001b[39moffset)\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:3392\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function\u001b[1;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[0;32m   3390\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[0;32m   3391\u001b[0m inputs, fn_args, additional_args, fn_kwargs \u001b[38;5;241m=\u001b[39m prepare_inputs(pa_inputs, indices, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[1;32m-> 3392\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39madditional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[0;32m   3393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "Cell \u001b[1;32mIn[14], line 47\u001b[0m, in \u001b[0;36mtokenize_and_align_labels\u001b[1;34m(examples)\u001b[0m\n\u001b[0;32m     45\u001b[0m     label_ids\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m word_id \u001b[38;5;241m!=\u001b[39m previous_word:\n\u001b[1;32m---> 47\u001b[0m     label_ids\u001b[38;5;241m.\u001b[39mappend(label_to_id[label[word_id]])\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# For subtokens, you can keep same label or set to -100\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     label_ids\u001b[38;5;241m.\u001b[39mappend(label_to_id[label[word_id]])\n",
      "\u001b[1;31mKeyError\u001b[0m: 16"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "from datasets import DatasetDict, ClassLabel, Sequence\n",
    "import numpy as np\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Define your NER labels\n",
    "# -------------------------------\n",
    "labels = [\"O\", \"B-SKILL\", \"I-SKILL\", \"B-COURSE\", \"I-COURSE\"]  # replace with your actual labels\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Convert ner_tags to ClassLabel\n",
    "# -------------------------------\n",
    "ner_feature = ClassLabel(names=labels)\n",
    "datasets = datasets.cast_column(\"ner_tags\", Sequence(ner_feature))\n",
    "\n",
    "# Now we can safely get label names and mappings\n",
    "label_list = datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "id_to_label = {i: l for i, l in enumerate(label_list)}\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Load pretrained Microsoft DeBERTa tokenizer\n",
    "# -------------------------------\n",
    "model_checkpoint = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Tokenize and align labels\n",
    "# -------------------------------\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels_batch = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word = None\n",
    "        label_ids = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_id != previous_word:\n",
    "                label_ids.append(label_to_id[label[word_id]])\n",
    "            else:\n",
    "                # For subtokens, you can keep same label or set to -100\n",
    "                label_ids.append(label_to_id[label[word_id]])\n",
    "            previous_word = word_id\n",
    "        labels_batch.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels_batch\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Load model\n",
    "# -------------------------------\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Define metrics\n",
    "# -------------------------------\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [\n",
    "        [id_to_label[l] for l in label if l != -100]\n",
    "        for label in labels\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [id_to_label[p] for (p, l) in zip(pred, label) if l != -100]\n",
    "        for pred, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Setup training\n",
    "# -------------------------------\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner-microsoft\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Train\n",
    "# -------------------------------\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b77a605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75eab4b112ef4b5bb2bc212265403b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Class label 16 greater than configured num_classes 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m     tokenized_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m labels_batch\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_inputs\n\u001b[1;32m---> 57\u001b[0m tokenized_datasets \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mmap(tokenize_and_align_labels, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# 3. Load model\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m     62\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     63\u001b[0m     model_checkpoint,\n\u001b[0;32m     64\u001b[0m     num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(label_list),\n\u001b[0;32m     65\u001b[0m     id2label\u001b[38;5;241m=\u001b[39mid_to_label,\n\u001b[0;32m     66\u001b[0m     label2id\u001b[38;5;241m=\u001b[39m{l: i \u001b[38;5;28;01mfor\u001b[39;00m i, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(label_list)}\n\u001b[0;32m     67\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\dataset_dict.py:902\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    899\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m    901\u001b[0m     {\n\u001b[1;32m--> 902\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m    903\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[0;32m    904\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[0;32m    905\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[0;32m    906\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[0;32m    907\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[0;32m    908\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m    909\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[0;32m    910\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[0;32m    911\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[0;32m    912\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[0;32m    913\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[0;32m    914\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[0;32m    915\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m    916\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[0;32m    917\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[0;32m    918\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[0;32m    919\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[0;32m    920\u001b[0m         )\n\u001b[0;32m    921\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    922\u001b[0m     }\n\u001b[0;32m    923\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    555\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    560\u001b[0m }\n\u001b[0;32m    561\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 562\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    563\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:3079\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3074\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3075\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3076\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3077\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3078\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3079\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3080\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3081\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:3534\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3532\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_table(batch\u001b[38;5;241m.\u001b[39mto_arrow())\n\u001b[0;32m   3533\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3534\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_batch(batch)\n\u001b[0;32m   3535\u001b[0m num_examples_progress_update \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_examples_in_batch\n\u001b[0;32m   3536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m _time \u001b[38;5;241m+\u001b[39m config\u001b[38;5;241m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\arrow_writer.py:605\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[1;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[0;32m    603\u001b[0m         col_try_type \u001b[38;5;241m=\u001b[39m try_features[col] \u001b[38;5;28;01mif\u001b[39;00m try_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m try_features \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    604\u001b[0m         typed_sequence \u001b[38;5;241m=\u001b[39m OptimizedTypedSequence(col_values, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mcol_type, try_type\u001b[38;5;241m=\u001b[39mcol_try_type, col\u001b[38;5;241m=\u001b[39mcol)\n\u001b[1;32m--> 605\u001b[0m         arrays\u001b[38;5;241m.\u001b[39mappend(pa\u001b[38;5;241m.\u001b[39marray(typed_sequence))\n\u001b[0;32m    606\u001b[0m         inferred_features[col] \u001b[38;5;241m=\u001b[39m typed_sequence\u001b[38;5;241m.\u001b[39mget_inferred_type()\n\u001b[0;32m    607\u001b[0m schema \u001b[38;5;241m=\u001b[39m inferred_features\u001b[38;5;241m.\u001b[39marrow_schema \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\pyarrow\\array.pxi:247\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\pyarrow\\array.pxi:112\u001b[0m, in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\arrow_writer.py:243\u001b[0m, in \u001b[0;36mTypedSequence.__arrow_array__\u001b[1;34m(self, type)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m# otherwise we can finally use the user's type\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;66;03m# We use cast_array_to_feature to support casting to custom types like Audio and Image\u001b[39;00m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;66;03m# Also, when trying type \"string\", we don't want to convert integers or floats to \"string\".\u001b[39;00m\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;66;03m# We only do it if trying_type is False - since this is what the user asks for.\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m         out \u001b[38;5;241m=\u001b[39m cast_array_to_feature(\n\u001b[0;32m    244\u001b[0m             out, \u001b[38;5;28mtype\u001b[39m, allow_primitive_to_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrying_type, allow_decimal_to_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrying_type\n\u001b[0;32m    245\u001b[0m         )\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;167;01mTypeError\u001b[39;00m,\n\u001b[0;32m    249\u001b[0m     pa\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39mArrowInvalid,\n\u001b[0;32m    250\u001b[0m     pa\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39mArrowNotImplementedError,\n\u001b[0;32m    251\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# handle type errors and overflows\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;66;03m# Ignore ArrowNotImplementedError caused by trying type, otherwise re-raise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\table.py:1797\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[1;34m(array, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mchunked_array([func(chunk, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m array\u001b[38;5;241m.\u001b[39mchunks])\n\u001b[0;32m   1796\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(array, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\table.py:2065\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[1;34m(array, feature, allow_primitive_to_str, allow_decimal_to_str)\u001b[0m\n\u001b[0;32m   2063\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mFixedSizeListArray\u001b[38;5;241m.\u001b[39mfrom_arrays(_c(array_values, feature\u001b[38;5;241m.\u001b[39mfeature), feature\u001b[38;5;241m.\u001b[39mlength)\n\u001b[0;32m   2064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2065\u001b[0m     casted_array_values \u001b[38;5;241m=\u001b[39m _c(array\u001b[38;5;241m.\u001b[39mvalues, feature\u001b[38;5;241m.\u001b[39mfeature)\n\u001b[0;32m   2066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_list(array\u001b[38;5;241m.\u001b[39mtype) \u001b[38;5;129;01mand\u001b[39;00m casted_array_values\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m array\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtype:\n\u001b[0;32m   2067\u001b[0m         \u001b[38;5;66;03m# Both array and feature have equal list type and values (within the list) type\u001b[39;00m\n\u001b[0;32m   2068\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m array\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\table.py:1797\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[1;34m(array, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mchunked_array([func(chunk, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m array\u001b[38;5;241m.\u001b[39mchunks])\n\u001b[0;32m   1796\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(array, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\table.py:1995\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[1;34m(array, feature, allow_primitive_to_str, allow_decimal_to_str)\u001b[0m\n\u001b[0;32m   1993\u001b[0m     array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mstorage\n\u001b[0;32m   1994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(feature, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcast_storage\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1995\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mcast_storage(array)\n\u001b[0;32m   1997\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_struct(array\u001b[38;5;241m.\u001b[39mtype):\n\u001b[0;32m   1998\u001b[0m     \u001b[38;5;66;03m# feature must be a dict or Sequence(subfeatures_dict)\u001b[39;00m\n\u001b[0;32m   1999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature, Sequence) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature\u001b[38;5;241m.\u001b[39mfeature, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\datasets\\features\\features.py:1130\u001b[0m, in \u001b[0;36mClassLabel.cast_storage\u001b[1;34m(self, storage)\u001b[0m\n\u001b[0;32m   1128\u001b[0m     min_max \u001b[38;5;241m=\u001b[39m pc\u001b[38;5;241m.\u001b[39mmin_max(storage)\u001b[38;5;241m.\u001b[39mas_py()\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m min_max[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m min_max[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1131\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass label \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_max[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m greater than configured num_classes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1132\u001b[0m         )\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(storage, pa\u001b[38;5;241m.\u001b[39mStringArray):\n\u001b[0;32m   1134\u001b[0m     storage \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m   1135\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strval2int(label) \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m storage\u001b[38;5;241m.\u001b[39mto_pylist()]\n\u001b[0;32m   1136\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Class label 16 greater than configured num_classes 5"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "from datasets import ClassLabel, Sequence\n",
    "import numpy as np\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Ensure ner_tags are ClassLabel\n",
    "# -------------------------------\n",
    "# This will automatically use the integer IDs already in your dataset\n",
    "if not isinstance(datasets[\"train\"].features[\"ner_tags\"].feature, ClassLabel):\n",
    "    # If it's still Value, convert it manually\n",
    "    unique_labels = set()\n",
    "    for ex in datasets[\"train\"]:\n",
    "        unique_labels.update(ex[\"ner_tags\"])\n",
    "    unique_labels = sorted(list(unique_labels))\n",
    "    ner_feature = ClassLabel(names=[str(i) for i in unique_labels])\n",
    "    datasets = datasets.cast_column(\"ner_tags\", Sequence(ner_feature))\n",
    "\n",
    "# Get label list and mappings\n",
    "label_list = datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "id_to_label = {i: l for i, l in enumerate(label_list)}\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load tokenizer\n",
    "# -------------------------------\n",
    "model_checkpoint = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Tokenize & align labels\n",
    "# -------------------------------\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels_batch = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word = None\n",
    "        label_ids = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_id != previous_word:\n",
    "                label_ids.append(label[word_id])  # directly use dataset IDs\n",
    "            else:\n",
    "                label_ids.append(label[word_id])\n",
    "            previous_word = word_id\n",
    "        labels_batch.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels_batch\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load model\n",
    "# -------------------------------\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,\n",
    "    label2id={l: i for i, l in enumerate(label_list)}\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Metrics\n",
    "# -------------------------------\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [\n",
    "        [id_to_label[l] for l in label if l != -100]\n",
    "        for label in labels\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [id_to_label[p] for (p, l) in zip(pred, label) if l != -100]\n",
    "        for pred, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Training setup\n",
    "# -------------------------------\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner-microsoft\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Train\n",
    "# -------------------------------\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e59f6b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers version: 4.51.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3c73ef93c74a288d6d39e56989347b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc65f13b6bf84fe99e5ecad15218bd91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falling back to legacy TrainingArguments parameters because: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluate_during_training'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 131\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 131\u001b[0m     training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcommon_args,\n\u001b[0;32m    133\u001b[0m         evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,   \u001b[38;5;66;03m# modern arg\u001b[39;00m\n\u001b[0;32m    134\u001b[0m         save_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,         \u001b[38;5;66;03m# modern arg\u001b[39;00m\n\u001b[0;32m    135\u001b[0m     )\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing modern TrainingArguments (evaluation_strategy/save_strategy).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 142\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# choose a reasonable save_steps fallback\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     fallback_save_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m--> 142\u001b[0m     training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcommon_args,\n\u001b[0;32m    144\u001b[0m         do_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    145\u001b[0m         evaluate_during_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# legacy arg name\u001b[39;00m\n\u001b[0;32m    146\u001b[0m         save_steps\u001b[38;5;241m=\u001b[39mfallback_save_steps,\n\u001b[0;32m    147\u001b[0m     )\n\u001b[0;32m    149\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m    150\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    151\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    156\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m    157\u001b[0m )\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# 6. Train\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluate_during_training'"
     ]
    }
   ],
   "source": [
    "# Full NER training script with compatibility fallback for older transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import transformers\n",
    "from datasets import ClassLabel, Sequence\n",
    "import numpy as np\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"transformers version:\", transformers.__version__)\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Ensure ner_tags are ClassLabel\n",
    "# -------------------------------\n",
    "if not isinstance(datasets[\"train\"].features[\"ner_tags\"].feature, ClassLabel):\n",
    "    unique_labels = set()\n",
    "    for ex in datasets[\"train\"]:\n",
    "        unique_labels.update(ex[\"ner_tags\"])\n",
    "    unique_labels = sorted(list(unique_labels))\n",
    "    ner_feature = ClassLabel(names=[str(i) for i in unique_labels])\n",
    "    datasets = datasets.cast_column(\"ner_tags\", Sequence(ner_feature))\n",
    "\n",
    "# Label mappings\n",
    "label_list = datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "id_to_label = {i: l for i, l in enumerate(label_list)}\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load tokenizer\n",
    "# -------------------------------\n",
    "model_checkpoint = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Tokenize & align labels\n",
    "# -------------------------------\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels_batch = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word = None\n",
    "        label_ids = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_id != previous_word:\n",
    "                label_ids.append(label[word_id])\n",
    "            else:\n",
    "                orig_label = id_to_label[label[word_id]]\n",
    "                if orig_label.startswith(\"B-\"):\n",
    "                    new_label = \"I-\" + orig_label[2:]\n",
    "                    label_ids.append(label_to_id.get(new_label, label[word_id]))\n",
    "                else:\n",
    "                    label_ids.append(label[word_id])\n",
    "            previous_word = word_id\n",
    "        labels_batch.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels_batch\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load model\n",
    "# -------------------------------\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Metrics (robust to types)\n",
    "# -------------------------------\n",
    "def compute_metrics(p):\n",
    "    # p may be (preds, labels) or an EvalPrediction with .predictions and .label_ids\n",
    "    if hasattr(p, \"predictions\"):\n",
    "        predictions = p.predictions\n",
    "        labels = p.label_ids\n",
    "    else:\n",
    "        predictions, labels = p\n",
    "\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [\n",
    "        [id_to_label[l] for l in label if l != -100]\n",
    "        for label in labels\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [id_to_label[p] for (p, l) in zip(pred, label) if l != -100]\n",
    "        for pred, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Training setup (compatibility)\n",
    "# -------------------------------\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Common args (used in both new and fallback versions)\n",
    "common_args = dict(\n",
    "    output_dir=\"./ner-microsoft\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Try using modern TrainingArguments parameters; fallback on TypeError to legacy names\n",
    "try:\n",
    "    training_args = TrainingArguments(\n",
    "        **common_args,\n",
    "        evaluation_strategy=\"epoch\",   # modern arg\n",
    "        save_strategy=\"epoch\",         # modern arg\n",
    "    )\n",
    "    print(\"Using modern TrainingArguments (evaluation_strategy/save_strategy).\")\n",
    "except TypeError as e:\n",
    "    # Fallback for older transformers that don't accept evaluation_strategy\n",
    "    print(\"Falling back to legacy TrainingArguments parameters because:\", e)\n",
    "    # choose a reasonable save_steps fallback\n",
    "    fallback_save_steps = 500\n",
    "    training_args = TrainingArguments(\n",
    "        **common_args,\n",
    "        do_eval=True,\n",
    "        evaluate_during_training=True,  # legacy arg name\n",
    "        save_steps=fallback_save_steps,\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Train\n",
    "# -------------------------------\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "920505be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\gayat\\anaconda3\\lib\\site-packages (4.56.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\gayat\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: seqeval in c:\\users\\gayat\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from seqeval) (1.5.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 3.3.2\n",
      "    Uninstalling datasets-3.3.2:\n",
      "      Successfully uninstalled datasets-3.3.2\n",
      "Successfully installed datasets-4.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade transformers datasets seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15247030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers: 4.51.3\n",
      "Python exec: c:\\Users\\gayat\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import transformers, sys\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Python exec:\", sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae5a4ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:10: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:36: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:36: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\gayat\\AppData\\Local\\Temp\\ipykernel_29452\\3368808822.py:10: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  skills = load_list(\"G:\\My Drive\\projects\\data_science\\working\\dataset\\skills_correction.csv\")\n",
      "C:\\Users\\gayat\\AppData\\Local\\Temp\\ipykernel_29452\\3368808822.py:11: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  degrees = load_list(\"g:\\My Drive\\projects\\data_science\\working\\dataset\\degrees.csv\")\n",
      "C:\\Users\\gayat\\AppData\\Local\\Temp\\ipykernel_29452\\3368808822.py:12: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  courses = load_list(\"g:\\My Drive\\projects\\data_science\\working\\dataset\\courses.csv\")\n",
      "C:\\Users\\gayat\\AppData\\Local\\Temp\\ipykernel_29452\\3368808822.py:13: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  disciplines = load_list(\"G:\\My Drive\\projects\\data_science\\working\\dataset\\disciplines.csv\")\n",
      "C:\\Users\\gayat\\AppData\\Local\\Temp\\ipykernel_29452\\3368808822.py:36: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  df = pd.read_csv(\"G:\\My Drive\\projects\\data_science\\working\\dataset\\internship_descriptions_20000.csv\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b4aa357c794970b33c0f4338d4fbe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0398c515b9534326827b746311c6db84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac0945c05e0441794b3826e555418d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25083045fcc425e83e147c30c41d4a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['commerce', '-', 'woo', '##com', '##mer', '##ce', 'intern', 'assist', 'in', 'data', 'collection', 'and', 'prep', '##ro', '##ces', '##sing', 'for', 'forensic', 'accounting', 'and', 'financial', 'fraud', 'investigation', 'applications', 'participate', 'in', 'code', 'reviews', 'and', 'provide', 'constructive', 'feedback', 'present', 'findings', 'and', 'project', 'outcomes', 'to', 'stakeholders', 'mentor', 'junior', 'intern', '##s', 'in', 'ao', 'engagement', 'basics', 'document', 'project', 'progress', 'and', 'technical', 'findings', 'support', 'deployment', 'and', 'integration', 'of', 'solutions', 'using', 'new', 'business', 'assist', 'in', 'hands', '-', 'on', 'coding', 'and', 'implementation', 'using', 'ad', '##words', 'collaborate', 'with', 'team', 'members', 'on', 'buddhist', 'studies', 'projects', 'contribute', 'to', 'project', 'planning', 'and', 'timeline', 'estimation', 'work', 'on', 'real', '-', 'world', 'case', 'studies', 'in', 'buddhist', 'studies', 'eager', '##ness', 'to', 'learn', 'new', 'technologies', 'and', 'methods', 'strong', 'knowledge', 'of', 'ad', '##words', ',', 'new', 'business', ',', 'and', 'ao', 'engagement', 'excellent', 'written', 'and', 'verbal', 'communication', 'skills', 'problem', '-', 'solving', 'skills', 'and', 'attention', 'to', 'detail', 'familiarity', 'with', 'industry', 'tools', 'such', 'as', 'bam', '##s', 'experience', 'with', 'project', 'work', 'or', 'internship', '##s', 'in', 'buddhist', 'studies', 'course', '##work', 'in', 'forensic', 'accounting', 'and', 'financial', 'fraud', 'investigation', 'domain', '-', 'specific', 'knowledge', 'in', 'forensic', 'accounting', 'and', 'financial', 'fraud', 'investigation']\n",
      "NER Tags: [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- Load helper lists (skills, degrees, etc.) ---\n",
    "def load_list(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "skills = load_list(\"G:\\My Drive\\projects\\data_science\\working\\dataset\\skills_correction.csv\")\n",
    "degrees = load_list(\"g:\\My Drive\\projects\\data_science\\working\\dataset\\degrees.csv\")\n",
    "courses = load_list(\"g:\\My Drive\\projects\\data_science\\working\\dataset\\courses.csv\")\n",
    "disciplines = load_list(\"G:\\My Drive\\projects\\data_science\\working\\dataset\\disciplines.csv\")\n",
    "\n",
    "# --- Tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# --- Simple NER tagging function ---\n",
    "def tag_text(text, skills, degrees, courses, disciplines):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    ner_tags = [0] * len(tokens)  # default \"O\"\n",
    "\n",
    "    for i, tok in enumerate(tokens):\n",
    "        word = tok.lower()\n",
    "        if word in skills:\n",
    "            ner_tags[i] = 1\n",
    "        elif word in degrees:\n",
    "            ner_tags[i] = 3\n",
    "        elif word in courses:\n",
    "            ner_tags[i] = 4\n",
    "        elif word in disciplines:\n",
    "            ner_tags[i] = 7\n",
    "    return {\"tokens\": tokens, \"ner_tags\": ner_tags}\n",
    "\n",
    "# --- Load ONE description only ---\n",
    "df = pd.read_csv(\"G:\\My Drive\\projects\\data_science\\working\\dataset\\internship_descriptions_20000.csv\")\n",
    "\n",
    "# Take just the first row (combine role + responsibilities + requirements)\n",
    "row = df.iloc[0]\n",
    "text = f\"{row['role']} {row['responsibilities']} {row['requirements']}\"\n",
    "\n",
    "# --- Tagging ---\n",
    "sample = tag_text(text, skills, degrees, courses, disciplines)\n",
    "\n",
    "print(\"Tokens:\", sample[\"tokens\"])\n",
    "print(\"NER Tags:\", sample[\"ner_tags\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
