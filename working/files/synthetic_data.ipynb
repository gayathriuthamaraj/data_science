{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "406c077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "802b9fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset saved to: G:\\My Drive\\projects\\data_science\\working\\dataset\\student_profiles_updated.csv\n",
      "         Name  10th marks 12th course  12th marks grad course  \\\n",
      "0  AMIRSHAYAN        62.9         NaN         NaN         NaN   \n",
      "1     SOANJAN        93.4        Arts        85.0        BCom   \n",
      "2     VAISHMI        65.6        Arts        69.4       BTech   \n",
      "3    DEDEEPYA        76.2    Commerce        69.4         NaN   \n",
      "4      HAMESH        86.2     Science        89.7         NaN   \n",
      "\n",
      "              grad stream  grad marks                skills  \\\n",
      "0                     NaN         NaN                   NaN   \n",
      "1  Electrical Engineering        71.8  Excel, Communication   \n",
      "2  Mechanical Engineering        65.2                   NaN   \n",
      "3                     NaN         NaN                   NaN   \n",
      "4                     NaN         NaN                   NaN   \n",
      "\n",
      "           languages known             internships taken  \\\n",
      "0  Punjabi, English, Hindi                           NaN   \n",
      "1       Urdu, Hindi, Tamil                           NaN   \n",
      "2                      NaN                           NaN   \n",
      "3          Tamil, Gujarati                           NaN   \n",
      "4                      NaN  Finance Intern at Local Firm   \n",
      "\n",
      "                        additional certifications  \n",
      "0  Certificate in IFRS, Oracle Java Certification  \n",
      "1                                             NaN  \n",
      "2                                             NaN  \n",
      "3                                             NaN  \n",
      "4                                             NaN  \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "names_file = r\"G:\\My Drive\\projects\\data_science\\working\\dataset\\IndianNamesUnique.csv\"\n",
    "profiles_file = r\"G:\\My Drive\\projects\\data_science\\working\\dataset\\student_profiles_education_levels_50000.csv\"\n",
    "\n",
    "names_df = pd.read_csv(names_file)\n",
    "profiles_df = pd.read_csv(profiles_file)\n",
    "\n",
    "if len(names_df) < len(profiles_df):\n",
    "    raise ValueError(\"Not enough names in the names dataset to replace all profiles!\")\n",
    "\n",
    "unique_names = random.sample(list(names_df['Name']), len(profiles_df))\n",
    "\n",
    "profiles_df['Name'] = unique_names\n",
    "\n",
    "output_file = r\"G:\\My Drive\\projects\\data_science\\working\\dataset\\student_profiles_updated.csv\"\n",
    "profiles_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Updated dataset saved to: {output_file}\")\n",
    "print(profiles_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a55dd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset with DOB saved to: G:\\My Drive\\projects\\data_science\\working\\dataset\\student_profiles_updated.csv\n",
      "            Name         DOB\n",
      "0       SATYAPAL  2001-08-17\n",
      "1       ARDASHIR  2002-12-30\n",
      "2        DEEPANA  2004-06-28\n",
      "3     NARABOYANA  2004-03-07\n",
      "4        RAEMELA  2001-12-14\n",
      "5       SUGASANI  2002-06-22\n",
      "6     RISHIKESAN  2003-09-14\n",
      "7        HARDYAL  2004-02-06\n",
      "8     JANAANJALI  2002-05-05\n",
      "9  KIRANDEEPKAUR  2004-04-03\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# File paths\n",
    "names_file = r\"G:\\My Drive\\projects\\data_science\\working\\dataset\\IndianNamesUnique.csv\"\n",
    "profiles_file = r\"G:\\My Drive\\projects\\data_science\\working\\dataset\\student_profiles_education_levels_50000.csv\"\n",
    "\n",
    "# Load datasets\n",
    "names_df = pd.read_csv(names_file)\n",
    "profiles_df = pd.read_csv(profiles_file)\n",
    "\n",
    "# Ensure enough names\n",
    "if len(names_df) < len(profiles_df):\n",
    "    raise ValueError(\"Not enough names in the names dataset to replace all profiles!\")\n",
    "\n",
    "# Random unique names\n",
    "unique_names = random.sample(list(names_df['Name']), len(profiles_df))\n",
    "\n",
    "# Replace names\n",
    "profiles_df['Name'] = unique_names\n",
    "\n",
    "# Function to generate DOB for age between 21 and 25\n",
    "def generate_random_dob(min_age=21, max_age=25):\n",
    "    today = datetime.today()\n",
    "    start_date = today.replace(year=today.year - max_age)\n",
    "    end_date = today.replace(year=today.year - min_age)\n",
    "    random_days = random.randint(0, (end_date - start_date).days)\n",
    "    return (start_date + timedelta(days=random_days)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Add DOB column\n",
    "profiles_df['DOB'] = [generate_random_dob() for _ in range(len(profiles_df))]\n",
    "\n",
    "# Save the updated dataset\n",
    "output_file = r\"G:\\My Drive\\projects\\data_science\\working\\dataset\\student_profiles_updated.csv\"\n",
    "profiles_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Updated dataset with DOB saved to: {output_file}\")\n",
    "print(profiles_df[['Name', 'DOB']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99a4f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset with DOB and Category saved to: G:\\My Drive\\projects\\data_science\\working\\dataset\\student_profiles_updated.csv\n",
      "           Name         DOB Category\n",
      "0       YOVITHA  2001-05-21      MBC\n",
      "1      SUKUNNAN  2004-08-18       BC\n",
      "2      SHOORSEN  2003-06-30       BC\n",
      "3   AADALALAGAN  2001-07-25       BC\n",
      "4  SKANDAPRASAD  2002-10-06      MBC\n",
      "5        SEHKAR  2001-12-28       BC\n",
      "6         AMISH  2001-06-10       BC\n",
      "7         HINDI  2001-11-27       BC\n",
      "8   ISAINILAVAN  2002-03-06       SC\n",
      "9   ILLAMURUGAN  2003-02-11       BC\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "names_file = r\"G:\\My Drive\\projects\\data_science\\working\\dataset\\IndianNamesUnique.csv\"\n",
    "profiles_file = r\"G:\\My Drive\\projects\\data_science\\working\\dataset\\student_profiles_education_levels_50000.csv\"\n",
    "\n",
    "names_df = pd.read_csv(names_file)\n",
    "profiles_df = pd.read_csv(profiles_file)\n",
    "\n",
    "if len(names_df) < len(profiles_df):\n",
    "    raise ValueError(\"Not enough names in the names dataset to replace all profiles!\")\n",
    "\n",
    "unique_names = random.sample(list(names_df['Name']), len(profiles_df))\n",
    "\n",
    "profiles_df['Name'] = unique_names\n",
    "\n",
    "def generate_random_dob(min_age=21, max_age=25):\n",
    "    today = datetime.today()\n",
    "    start_date = today.replace(year=today.year - max_age)\n",
    "    end_date = today.replace(year=today.year - min_age)\n",
    "    random_days = random.randint(0, (end_date - start_date).days)\n",
    "    return (start_date + timedelta(days=random_days)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "profiles_df['DOB'] = [generate_random_dob() for _ in range(len(profiles_df))]\n",
    "\n",
    "categories = [\"OC\", \"BC\", \"MBC\", \"SC\", \"ST\"]\n",
    "probabilities = [0.10, 0.45, 0.25, 0.18, 0.02]\n",
    "profiles_df['Category'] = random.choices(categories, probabilities, k=len(profiles_df))\n",
    "\n",
    "output_file = r\"G:\\My Drive\\projects\\data_science\\working\\dataset\\student_profiles_updated.csv\"\n",
    "profiles_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Updated dataset with DOB and Category saved to: {output_file}\")\n",
    "print(profiles_df[['Name', 'DOB', 'Category']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d1cf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\gayat\\anaconda3\\lib\\site-packages (1.61.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c136486",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m----> 2\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[0;32m      4\u001b[0m emb1 \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a test\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-embedding-3-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39membedding\n\u001b[0;32m      5\u001b[0m emb2 \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is an experiment\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-embedding-3-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39membedding\n",
      "File \u001b[1;32mc:\\Users\\gayat\\anaconda3\\Lib\\site-packages\\openai\\_client.py:110\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    108\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "emb1 = client.embeddings.create(input=\"This is a test\", model=\"text-embedding-3-large\").data[0].embedding\n",
    "emb2 = client.embeddings.create(input=\"This is an experiment\", model=\"text-embedding-3-large\").data[0].embedding\n",
    "\n",
    "similarity = cosine_similarity(emb1, emb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "564a0990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\gayat\\anaconda3\\lib\\site-packages (4.51.3)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\gayat\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\gayat\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-5.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers sentence-transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ec7200e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5809, 0.2172],\n",
      "        [0.0577, 0.4790]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "candidates = [\"Python, Machine Learning\", \"Java, Data Structures\"]\n",
    "internships = [\"Looking for ML and Python experience\", \"Java developer needed\"]\n",
    "\n",
    "candidate_embeddings = model.encode(candidates, convert_to_tensor=True)\n",
    "internship_embeddings = model.encode(internships, convert_to_tensor=True)\n",
    "\n",
    "similarity_matrix = util.cos_sim(candidate_embeddings, internship_embeddings)\n",
    "print(similarity_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
